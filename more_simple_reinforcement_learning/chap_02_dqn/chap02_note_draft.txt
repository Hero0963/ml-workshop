DQN = deep QLearning
QTable 的侷限: 當動作 or 狀態數 無法窮舉時
從 QTable 到神經網路: state → model → Q(values for all actions)，相當於每次查詢輸出 QTable 的 1 row

# 平衡車遊戲介紹
一局有200個時刻，桿子會往左右傾倒，車子必須保持移動來維持平衡
只要有一個時刻桿子不倒就 +1分，只要有一個時刻倒了，就 -1000分
-1000 <= total_score <= 200

DQN double model: target 延遲更新，才不會一直追逐目標。


# Using a PyTorch index
https://docs.astral.sh/uv/guides/integration/pytorch/#using-a-pytorch-index


# model_delay

'''
你做了一個「大腦」，輸入是4個數字（代表桿子角度、位置等），輸出是「左」或「右」兩個動作的分數。

還有一個一模一樣但不常更新的大腦（target network），這個是訓練的關鍵——避免因為太快自我複製學壞。
'''


加權數據池（weighted replay buffer / prioritized replay buffer）
對每條 transition（狀態-動作-回報-下個狀態）賦予一個權重，通常根據該樣本的 TD-error（時間差誤差，簡單來說就是loss），
誤差大的 transition 會被賦予更高的抽樣概率。

为了缓解过拟合,降低这些数据的lr,这里以削减loss的方式实现.
per-sample adaptive adjustment

====================================

| 特性         | 算法 A：延迟网络 + max                                     | 算法 B：Double DQN（Decoupled）                                               |
|--------------|-----------------------------------------------------------|------------------------------------------------------------------------------|
| 网络个数     | 2（主/延迟）                                              | 2（主/延迟）                                                                 |
| 动作选择     | 直接在延迟网络上选取最大 Q 值                              | 在主网络上选取最大 Q 值                                                       |
| 动作评估     | 同样用延迟网络评估该最大值                                | 用延迟网络评估“主网络选出的”动作                                             |
| 目标公式     | \(y = r + \gamma \max_{a'} Q(s',a';\theta^-)\)             | \(y = r + \gamma\,Q\bigl(s',\arg\max_{a'}Q(s',a';\theta),\theta^-\bigr)\)     |
| 解决的问题   | 缓解了自举不稳定（target 网络延迟）                       | 进一步解决了“最大化偏差”（maximization bias）                               |
