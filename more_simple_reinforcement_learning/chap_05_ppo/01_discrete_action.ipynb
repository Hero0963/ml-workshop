{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04dca527",
   "metadata": {},
   "source": [
    "状态价值函数:\n",
    "\n",
    "V(state) = 所有动作求和 -> 概率(action) * Q(state,action)\n",
    "\n",
    "对这个式子做变形得到:\n",
    "\n",
    "V(state) = 所有动作求和 -> 现概率(action) * [旧概率(action) / 现概率(action)] * Q(state,action)\n",
    "\n",
    "初始时可以认为现概率和旧概率相等,但随着模型的更新,现概率会变化.\n",
    "\n",
    "式子中的Q(state,action)可以用蒙特卡洛法估计.\n",
    "\n",
    "按照策略梯度的理论,状态价值取决于动作的质量,所以只要最大化V函数,就可以得到最好的动作策略."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6d0433a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "#定义环境\n",
    "class MyWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self):\n",
    "        env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.step_n = 0\n",
    "\n",
    "    def reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        self.step_n = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        over = terminated or truncated\n",
    "\n",
    "        #限制最大步数\n",
    "        self.step_n += 1\n",
    "        if self.step_n >= 200:\n",
    "            over = True\n",
    "\n",
    "        #没坚持到最后,扣分\n",
    "        if over and self.step_n < 200:\n",
    "            reward = -1000\n",
    "\n",
    "        return state, reward, over\n",
    "\n",
    "    #打印游戏图像\n",
    "    def show(self):\n",
    "        from matplotlib import pyplot as plt\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(self.env.render())\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()\n",
    "\n",
    "env.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd13f3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5005, 0.4995],\n",
       "         [0.4786, 0.5214]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0713],\n",
       "         [0.2155]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#定义模型\n",
    "model_action = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 2),\n",
    "    torch.nn.Softmax(dim=1),\n",
    ")\n",
    "\n",
    "model_value = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 1),\n",
    ")\n",
    "\n",
    "model_action(torch.randn(2, 4)), model_value(torch.randn(2, 4))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "  高中生版本解說\n",
    "\n",
    "  這個 Cell 在做的事情是「創造兩個大腦」，一個用來「做決定」，另一個用來「打分數」。這就是 Actor-Critic (演員-評論家) 模式。\n",
    "\n",
    "   1. `model_action` (演員 / 決策腦):\n",
    "       * 功能：這是負責實際操作的「演員」。它觀察遊戲的當前狀態（一個包含 4 個數字的列表，描述車和杆子的位置、速度等），然後輸出一個決策。\n",
    "       * 輸出：它的輸出是兩個數字，代表「向左走的機率」和「向右走的機率」。例如，它可能會輸出 [0.7, 0.3]，意思是「我有 70% 的把握應該向左，30%\n",
    "         的把握應該向右」。最後的 Softmax 確保了這兩個機率加起來剛好等於 1。\n",
    "\n",
    "   2. `model_value` (評論家 / 評分腦):\n",
    "       * 功能：這是負責在旁邊觀看和評價的「評論家」。它也觀察同樣的遊戲狀態。\n",
    "       * 輸出：它只輸出一個數字。這個數字代表了它對當前局勢的評分，即「狀態價值 (State Value)」。如果分數很高，代表評論家認為「現在這個局面很\n",
    "         好，未來很有可能獲得高分」。如果分數很低，則代表「現在情況不妙，未來可能會輸」。\n",
    "\n",
    "\n",
    "  最後一行程式碼 model_action(...) 和 model_value(...) 只是在測試一下，確保我們創造的這兩個大腦的輸入和輸出格式都沒問題。\n",
    "\n",
    "  簡單來說，我們讓一個大腦專心學習「如何行動」，另一個大腦專心學習「如何判斷局勢」，然後讓它們合作來玩遊戲。\n",
    "\n",
    "  專業術語解說\n",
    "\n",
    "  這個 Cell 定義了 PPO 演算法所需的兩個核心神經網路：策略網路 (Actor) 和價值網路 (Critic)。\n",
    "\n",
    "   1. `model_action` (Actor - 策略網路 $\\pi_\\theta(a|s)$):\n",
    "       * 架構: 這是一個標準的多層感知機 (MLP)。它使用 torch.nn.Sequential 容器來搭建。\n",
    "       * 輸入: 狀態 (state)，維度為 4。\n",
    "       * 輸出: 一個維度為 2 的向量，代表了在該狀態下，對應兩個離散動作（0: 向左, 1: 向右）的機率分佈。\n",
    "       * `Softmax` 激活函數: 輸出層之後的 Softmax 層是至關重要的。它將網路輸出的原始分數 (logits)\n",
    "         轉換成一個合法的機率分佈（所有元素非負，且總和為 1）。這個機率分佈後續將被用來創建一個 Categorical 分佈對象，以便從中採樣動作。\n",
    "\n",
    "   2. `model_value` (Critic - 價值網路 $V_\\phi(s)$):\n",
    "       * 架構: 同樣是一個 MLP，其隱藏層結構與 Actor 非常相似。\n",
    "       * 輸入: 狀態 (state)，維度為 4。\n",
    "       * 輸出: 一個純量的、無界的值。這個值是對輸入狀態 s 的價值（Value）的估計，即\n",
    "         $V(s)$。它預測了從當前狀態開始，遵循當前策略，未來能夠獲得的總折扣獎勵的期望值。\n",
    "       * 線性輸出層: 注意，價值網路的輸出層 torch.nn.Linear(64, 1)\n",
    "         後面沒有激活函數。這是因為價值函數的取值範圍是沒有限制的（可以是任何正數或負數），所以我們需要一個線性的輸出層。\n",
    "\n",
    "  這種將 Actor 和 Critic 分別用獨立的網路來實現的架構是 Actor-Critic 方法的常見做法。雖然它們的輸入相同，但它們的學習目標和輸出截然不同。\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da24a9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\it_project\\github_sync\\ml-workshop\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_32392\\1112667714.py:34: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  state = torch.FloatTensor(state).reshape(-1, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-983.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "import random\n",
    "\n",
    "\n",
    "#玩一局游戏并记录数据\n",
    "def play(show=False):\n",
    "    state = []\n",
    "    action = []\n",
    "    reward = []\n",
    "    next_state = []\n",
    "    over = []\n",
    "\n",
    "    s = env.reset()\n",
    "    o = False\n",
    "    while not o:\n",
    "        #根据概率采样\n",
    "        prob = model_action(torch.FloatTensor(s).reshape(1, 4))[0].tolist()\n",
    "        a = random.choices(range(2), weights=prob, k=1)[0]\n",
    "\n",
    "        ns, r, o = env.step(a)\n",
    "\n",
    "        state.append(s)\n",
    "        action.append(a)\n",
    "        reward.append(r)\n",
    "        next_state.append(ns)\n",
    "        over.append(o)\n",
    "\n",
    "        s = ns\n",
    "\n",
    "        if show:\n",
    "            display.clear_output(wait=True)\n",
    "            env.show()\n",
    "\n",
    "    state = torch.FloatTensor(state).reshape(-1, 4)\n",
    "    action = torch.LongTensor(action).reshape(-1, 1)\n",
    "    reward = torch.FloatTensor(reward).reshape(-1, 1)\n",
    "    next_state = torch.FloatTensor(next_state).reshape(-1, 4)\n",
    "    over = torch.LongTensor(over).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over, reward.sum().item()\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over, reward_sum = play()\n",
    "\n",
    "reward_sum\n",
    "\n",
    "'''\n",
    "  高中生版本解說\n",
    "\n",
    "  這個 Cell 的作用就像一個「遊戲記錄員」。它定義了一個 play 函數，讓我們先前創建的「演員大腦\n",
    "  (model_action)」去實際玩一局平衡車遊戲，並把整個過程詳細地記錄下來。\n",
    "\n",
    "   1. 開始遊戲：遊戲從頭開始 (env.reset())。\n",
    "   2. 重複決策直到遊戲結束：\n",
    "       * 記錄員把當前情況（狀態 s）給演員大腦看。\n",
    "       * 演員大腦給出它的建議，比如 [0.7, 0.3]（70%機率向左，30%機率向右）。\n",
    "       * 記錄員不一定會選擇機率最高的那個，而是像「抽獎」一樣，根據這個機率分佈來隨機選擇一個動作\n",
    "         a。這很重要，因為這樣能讓演員偶爾嘗試一些不那麼有信心的動作，可能會發現新大陸（這叫「探索」）。\n",
    "       * 記錄員把選好的動作 a 告訴遊戲，遊戲更新到下一個狀態 ns，並給出得分 r。\n",
    "       * 記錄員把這一步的所有信息（當前狀態，執行的動作，得分，下一個狀態，遊戲是否結束）全都記在小本本上。\n",
    "   3. 整理報告：一局遊戲結束後，記錄員會把小本本上的所有數據整理成一個整齊的表格（PyTorch Tensors），方便後續的「教練（訓練函數）」進行分析。\n",
    "   4. 返回結果：最後，play 函數返回這份詳細的「遊戲報告」以及這一局的總得分。\n",
    "\n",
    "  最後兩行程式碼就是實際執行了一次 play 函數，並把這一局的總分顯示出來。\n",
    "\n",
    "  專業術語解說\n",
    "\n",
    "  這個 Cell 實現了強化學習中的數據收集 (Data Collection) 或 軌跡展開 (Trajectory Rollout) 階段。play 函數的核心職責是使用當前的策略\n",
    "  $\\pi_\\theta$（由 `model_action` 參數化）與環境互動，並收集一個完整的回合 (Episode) 的數據。\n",
    "\n",
    "   1. 策略執行:\n",
    "       * prob = model_action(...): 將當前狀態 s 輸入策略網路，得到動作的機率分佈。\n",
    "       * a = random.choices(...): 根據策略網路輸出的機率分佈，隨機採樣一個動作 a。這是執行一個隨機策略 (Stochastic Policy)\n",
    "         的標準方式。它確保了 Agent 在探索 (Exploration) 和利用 (Exploitation) 之間取得平衡。\n",
    "\n",
    "   2. 數據存儲: 函數將每一步的轉移元組 $(s_t, a_t, r_t, s_{t+1}, d_t)$（狀態, 動作, 獎勵, 下一狀態, 結束標誌）都存儲在 Python 列表中。\n",
    "\n",
    "   3. 數據批處理 (Batching):\n",
    "       * 在回合結束後，所有存儲在列表中的數據都被轉換為 PyTorch Tensor。\n",
    "       * reshape(-1, ...) 是一個關鍵操作，它將單步的數據組合成一個批次 (Batch)。例如，state 從一個包含多個 (4,) 陣列的列表，變成一個形狀為 [N,\n",
    "         4] 的張量，其中 N 是這個回合的總步數。\n",
    "       * 將數據轉換為批次張量，是為了利用 PyTorch 進行高效的向量化計算，這比在 Python 中用 for 迴圈處理每一步數據要快得多。\n",
    "\n",
    "   4. On-Policy 數據: play 函數返回的這一整套數據，是嚴格遵循當前策略 $\\pi_\\theta$ 產生的，因此被稱為 On-Policy 數據。PPO\n",
    "      演算法將使用這些數據來同時更新策略網路和價值網路。\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b952ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_action = torch.optim.Adam(model_action.parameters(), lr=1e-3)\n",
    "optimizer_value = torch.optim.Adam(model_value.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "def requires_grad(model, value):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(value)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "requires_grad 是一種「全局開關」，直接告訴 PyTorch：「接下來，請完全忽略這個模型的所有梯度計算」。\n",
    "\n",
    "  讓我們來看看幾種常見的替代寫法，以及它們的優劣。\n",
    "\n",
    "  ---\n",
    "\n",
    "  替代方案 1：使用 torch.no_grad() 上下文管理器 (最常見)\n",
    "\n",
    "  這是 PyTorch 中最慣用 (idiomatic)、最常見的寫法。\n",
    "\n",
    "  寫法：\n",
    "  在 train_value 函數中，計算 target 的部分已經用到了：\n",
    "\n",
    "   1 # 在 with torch.no_grad() 這個區塊內的所有計算，都不會被追蹤梯度\n",
    "   2 with torch.no_grad():\n",
    "   3     target = model_value(next_state)\n",
    "  如何替代 `requires_grad`：\n",
    "  我們可以擴大 no_grad 的使用範圍。當我們在訓練 model_value 時，任何涉及到 model_action 的計算（如果有的話）都可以被包裹在 with\n",
    "  torch.no_grad(): 中。\n",
    "\n",
    "  優點：\n",
    "   * 可讀性好：程式碼意圖非常清晰，讀者能一眼看出哪一段程式碼是不需要計算梯度的。\n",
    "   * 作用域明確：只在 with 區塊內生效，離開區塊後自動恢復，不容易出錯。\n",
    "\n",
    "  缺點：\n",
    "   * 如果一個函數裡有多處需要關閉梯度，可能會需要寫好幾個 with 區塊。\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a39ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_value(state, reward, next_state, over):\n",
    "    requires_grad(model_action, False)\n",
    "    requires_grad(model_value, True)\n",
    "\n",
    "    #计算target\n",
    "    with torch.no_grad():\n",
    "        target = model_value(next_state)\n",
    "    target = target * 0.98 * (1 - over) + reward\n",
    "\n",
    "    #每批数据反复训练10次\n",
    "    for _ in range(10):\n",
    "        #计算value\n",
    "        value = model_value(state)\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(value, target)\n",
    "        loss.backward()\n",
    "        optimizer_value.step()\n",
    "        optimizer_value.zero_grad()\n",
    "\n",
    "    #减去value相当于去基线\n",
    "    return (target - value).detach()\n",
    "\n",
    "\n",
    "value = train_value(state, reward, next_state, over)\n",
    "\n",
    "value.shape\n",
    "\n",
    "\n",
    "'''\n",
    "高中生版本解說\n",
    "\n",
    "  這個函數的目標是訓練我們的「評論家大腦 (`model_value`)」，讓它的打分越來越準。同時，它還會計算一個副產品，叫做「優勢\n",
    "  (Advantage)」，給下一個函數使用。\n",
    "\n",
    "   1. 計算「標準答案 (`target`)」：\n",
    "       * 評論家大腦的學習方式是「對答案」。所以我們首先要算出「標準答案」是什麼。\n",
    "       * 這裡的「標準答案」target 的計算方法是：你這一步的得分 (reward) + 0.98 * 評論家對你下一步狀態的評分。\n",
    "       * 這背後的思想是：一個狀態的「真實價值」，等於你立刻能拿到的獎勵，再加上你移動到的下一個狀態的「未來潛力」（潛力要打個折，這裡是 98\n",
    "         折）。這在強化學習裡叫 TD-Target。\n",
    "\n",
    "   2. 重複學習 10 次：\n",
    "       * PPO 的一個特點就是「溫故而知新」，它會拿著同一批遊戲數據，讓大腦反覆學習好幾次。這裡設置了學習 10 次。\n",
    "       * 在每一次學習中：\n",
    "           * 評論家對當前狀態做出自己的「預測 (value)」。\n",
    "           * 比較「預測」和「標準答案」之間的差距 (loss)。\n",
    "           * 根據這個差距，它的專屬教練 optimizer_value 會微調評論家大腦的參數，讓它的預測下次能更接近標準答案。\n",
    "\n",
    "   3. 計算並返回「優勢 (Advantage)」：\n",
    "       * 訓練 10 次結束後，函數會返回 (target - value)。這個值，就是優勢。\n",
    "       * 它的意義是：「你這一步的實際表現，比評論家預期的要好/差多少」。\n",
    "           * 如果 Advantage > 0：說明這一步走得好，實際結果超出了預期。\n",
    "           * 如果 Advantage < 0：說明這一步走得差，實際結果不如預期。\n",
    "       * 這個「優勢」值，將會被用來指導「演員大腦」該如何改進。.detach()\n",
    "         的意思是，我們把這個結果從計算圖中「剪」下來，把它當成一個固定的數字，再傳給下一個函數。\n",
    "\n",
    "  專業術語解說\n",
    "\n",
    "  這個 train_value 函數有兩個主要職責：1. 更新價值網路 (Critic) 的參數 $\\phi$。 2. 計算用於更新策略網路 (Actor) 的優勢函數 $A(s,a)$\n",
    "  的估計值。\n",
    "\n",
    "   1. TD-Target 計算:\n",
    "       * target = target * 0.98 * (1 - over) + reward 這行程式碼計算的是 時序差分目標 (Temporal Difference Target)。\n",
    "       * 其公式為 $Y_t = r_t + \\gamma \\cdot V_\\phi(s_{t+1}) \\cdot (1 - d_t)$，其中 $\\gamma=0.98$ 是折扣因子， $d_t$ 是結束標誌位 (over)。\n",
    "       * with torch.no_grad() 確保了在計算 $V_\\phi(s_{t+1})$ 時不會追蹤梯度，因為 $Y_t$ 在價值學習中應被視為一個固定的監督學習標籤。\n",
    "\n",
    "   2. 價值網路 (Critic) 的更新:\n",
    "       * 該函數的核心是一個迴圈，它使用同一批數據 (state, reward, next_state, over) 訓練價值網路 10 次。這是 PPO\n",
    "         的一個關鍵特徵，即每個數據批次進行多個 Epoch 的優化，以提高數據利用率。\n",
    "       * 損失函數: torch.nn.functional.mse_loss(value, target) 計算了預測價值 $V_\\phi(s_t)$ 和 TD-Target $Y_t$ 之間的均方誤差 (Mean Squared\n",
    "         Error, MSE)。Critic 的目標就是最小化這個誤差。\n",
    "       * 優化步驟: loss.backward(), optimizer_value.step(), optimizer_value.zero_grad() 是標準的 PyTorch\n",
    "         訓練步驟，用於計算梯度並更新價值網路的權重 $\\phi$。\n",
    "\n",
    "   3. 優勢函數 (Advantage Function) 估計:\n",
    "       * 函數最終返回 (target - value).detach()。這個量 $Y_t - V_\\phi(s_t)$ 是 **TD 誤差 (TD-Error)** $\\delta_t$。\n",
    "       * TD 誤差是優勢函數 $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$ 的一個無偏估計。在這個實現中，作者直接使用單步的 TD 誤差作為優勢的估計值來指導\n",
    "         Actor 的訓練。\n",
    "       * .detach() 方法至關重要。它創建了一個與當前計算圖分離的新張量。這可以防止在後續訓練 Actor 時，梯度錯誤地回傳到 Critic\n",
    "         網路中。我們希望將優勢值視為一個固定的數值常數（基線），而不是一個需要求導的變數。\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a15bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "539.3924560546875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_action(state, action, value):\n",
    "    requires_grad(model_action, True)\n",
    "    requires_grad(model_value, False)\n",
    "\n",
    "    #计算优势函数\n",
    "    delta = []\n",
    "    for i in range(len(value)):\n",
    "        s = 0\n",
    "        for j in range(i, len(value)):\n",
    "            s += value[j] * (0.98 * 0.95)**(j - i)\n",
    "        delta.append(s)\n",
    "    delta = torch.FloatTensor(delta).reshape(-1, 1)\n",
    "\n",
    "    #更新前的动作概率\n",
    "    with torch.no_grad():\n",
    "        prob_old = model_action(state).gather(dim=1, index=action)\n",
    "\n",
    "    #每批数据反复训练10次\n",
    "    for _ in range(10):\n",
    "        #更新后的动作概率\n",
    "        prob_new = model_action(state).gather(dim=1, index=action)\n",
    "\n",
    "        #求出概率的变化\n",
    "        ratio = prob_new / prob_old\n",
    "\n",
    "        #计算截断的和不截断的两份loss,取其中小的\n",
    "        surr1 = ratio * delta\n",
    "        surr2 = ratio.clamp(0.8, 1.2) * delta\n",
    "\n",
    "        loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        #更新参数\n",
    "        loss.backward()\n",
    "        optimizer_action.step()\n",
    "        optimizer_action.zero_grad()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "train_action(state, action, value)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "高中生版本解說\n",
    "\n",
    "  這個函數的目標是訓練「演員大腦 (`model_action`)」。它會拿到評論家算好的「優勢 (value)」這份報告，然後根據報告來改進自己的表演。\n",
    "\n",
    "   1. 計算更準確的「最終優勢 (`delta`)」：\n",
    "       * 函數拿到的 value 其實是每一步的「即時優勢」。但一個動作的好壞，不只看當下，還要看它對未來的長遠影響。\n",
    "       * 所以第一個 for 迴圈做的事情，就是把未來幾步的「即時優勢」加權求和，得到一個更穩定、更有遠見的「最終優勢\n",
    "         (delta)」。這就像評估一步棋，不僅要看它是否能立刻吃掉對方一個子，還要看它是否有利於後續的佈局。這個技巧叫做 GAE (廣義優勢估計)。\n",
    "\n",
    "   2. 記住舊策略 (`prob_old`)：\n",
    "       * 在開始訓練前，我們先用 with torch.no_grad() 把演員當前對每個動作的「自信程度（機率）」記錄下來，存到\n",
    "         prob_old。這相當於拍一張快照，作為後續比較的基準。\n",
    "\n",
    "   3. 反覆訓練 10 次，並使用 PPO 的「裁剪」魔法：\n",
    "       * 在迴圈中，我們不斷微調演員的策略。\n",
    "       * prob_new 是演員更新後的策略機率。\n",
    "       * ratio = prob_new / prob_old 是策略更新的幅度。如果 ratio > 1，代表演員對這個動作更有信心了。\n",
    "       * PPO 的核心思想來了：我們用兩種方式計算演員的「得分 (loss)」：\n",
    "           1. surr1：正常得分。ratio * delta，如果一個好動作（delta > 0）的機率提高了（ratio > 1），得分就高。\n",
    "           2. surr2：被裁剪過的「謹慎」得分。ratio.clamp(0.8, 1.2) 這一步是關鍵，它把 ratio 強行限制在 [0.8, 1.2]\n",
    "              的範圍內。這等於是給演員加了一個限制器：「你可以改進，但步子不能邁得太大！」\n",
    "       * loss = -torch.min(surr1,\n",
    "         surr2)：我們最終選擇兩種得分中較小的那一個來更新策略。這是一種「悲觀」策略，它確保了演員的每一次更新都是穩定、小步、且安全的。\n",
    "       * 這個「限制更新幅度」的裁剪操作，就是 PPO 演算法如此穩定且成功的秘訣。它有效避免了演員突然學到一個奇怪的策略而導致全盤崩潰。\n",
    "\n",
    "  專業術語解說\n",
    "\n",
    "  這個函數實現了 PPO-Clip 演算法的核心——策略網路的更新。\n",
    "\n",
    "   1. 廣義優勢估計 (Generalized Advantage Estimation, GAE):\n",
    "       * 函數接收的 value 參數實際上是前一個函數計算出的 TD-Error $\\delta_t$。\n",
    "       * 第一個 for 迴圈基於這些 TD-Error 計算了 GAE。其公式為 $\\hat{A}^{GAE}t = \\sum{l=0}^{T-t} (\\gamma\\lambda)^l \\delta_{t+l}$。\n",
    "       * 在這個程式碼中，折扣因子 $\\gamma$ 是 0.98，GAE 的 $\\lambda$ 參數是 0.95。\n",
    "       * 相比於單步的 TD-Error，GAE 通過引入 $\\lambda$ 參數，在無偏的蒙地卡羅估計和低變異數的 TD\n",
    "         估計之間做了一個權衡，通常能提供更穩定和可靠的優勢信號。\n",
    "\n",
    "   2. 重要性採樣比率 (Importance Sampling Ratio):\n",
    "       * prob_old 是在更新開始前，由策略 $\\pi_{\\theta_{old}}$ 計算出的採取動作 $a_t$ 的機率，它在 10 次更新迭代中保持不變。\n",
    "       * ratio = prob_new / prob_old 計算了核心的重要性採樣比率 $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$。\n",
    "\n",
    "   3. PPO 裁剪代理目標 (Clipped Surrogate Objective):\n",
    "       * PPO 的目標函數是：\n",
    "          $$ L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon,\n",
    "  1+\\epsilon) \\hat{A}_t \\right) \\right] $$\n",
    "       * surr1 = ratio * delta 對應了公式中的第一項 $r_t(\\theta) \\hat{A}_t$，這是標準的策略梯度目標。\n",
    "       * surr2 = ratio.clamp(0.8, 1.2) * delta 對應了第二項，其中 clamp(0.8, 1.2) 實現了 $\\text{clip}$ 函數，這裡的裁剪超參數 $\\epsilon$\n",
    "         被設定為 0.2。\n",
    "       * loss = -torch.min(surr1, surr2).mean() 計算了這個目標函數的期望值。torch.min 確保了更新是保守的，當策略變化過大（ratio 遠離\n",
    "         1）時，會被裁剪後的目標限制住，從而防止破壞性的策略更新。\n",
    "       * 最後的負號是因為 PyTorch 的優化器是做梯度下降，而我們的目標是最大化 $L^{CLIP}(\\theta)$。\n",
    "\n",
    "   4. 多 Epoch 更新: 與 train_value 類似，策略更新也在同一個數據批次上迭代了 10 次，這是 PPO 提高樣本效率的關鍵。\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "225817dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -15.426177978515625 -985.75\n",
      "100 -5.122213840484619 -846.4\n",
      "200 -12.664922714233398 200.0\n",
      "300 -11.645334243774414 200.0\n",
      "400 -10.312492370605469 200.0\n",
      "500 -8.571435928344727 200.0\n",
      "600 -6.572562217712402 200.0\n",
      "700 -6.021373271942139 200.0\n",
      "800 -4.376967906951904 200.0\n",
      "900 -2.203601837158203 200.0\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model_action.train()\n",
    "    model_value.train()\n",
    "\n",
    "    #训练N局\n",
    "    for epoch in range(1000):\n",
    "        #一个epoch最少玩N步\n",
    "        steps = 0\n",
    "        while steps < 200:\n",
    "            state, action, reward, next_state, over, _ = play()\n",
    "            steps += len(state)\n",
    "\n",
    "            #训练两个模型\n",
    "            delta = train_value(state, reward, next_state, over)\n",
    "            loss = train_action(state, action, delta)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            test_result = sum([play()[-1] for _ in range(20)]) / 20\n",
    "            print(epoch, loss, test_result)\n",
    "\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8745f470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASAAAADMCAYAAADTcn7NAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAATSklEQVR4nO3dbUxU158H8N/MMIwgDggKFIHF3bq11KcWEanZbVOp1JqmVl+0XWOpcTW1aHxoTEqiGI0Jxr6wtVWa/zZV31gbmtBGVm0IKKbrWBTLRlHZdreNRB2mPsyAKDPMzNn8zj9zyyAqyDhn7sz3k1yv994zw5nLzJdzz7l3rkEIIQgAQAGjih8KAMAQQACgDAIIAJRBAAGAMgggAFAGAQQAyiCAAEAZBBAAKIMAAgBlEEAAEHsBtGfPHsrLy6NRo0ZRUVERNTc3q6oKAMRSAH377be0YcMG2rJlC507d46mT59OpaWl5HA4VFQHABQxqLgYlVs8hYWF9MUXX8hlv99POTk5tGbNGvr444/DXR0AUCQu3D/Q4/FQS0sLVVRUaOuMRiOVlJSQzWYb9DFut1tOARxYt27dorS0NDIYDGGpNwAMHbdruru7KSsrS36+IyaAbty4QT6fjzIyMoLW8/Lly5cHfUxVVRVt3bo1TDUEgFDp6Oig7OzsyAmgx8GtJe4zCnC5XJSbmytfnNVqVVo3ALhfV1eX7FYZM2YMPUzYA2jcuHFkMpmos7MzaD0vZ2ZmDvoYi8Uip4E4fBBAAJHrUV0kYR8Fi4+Pp4KCAmpoaAjq0+Hl4uLicFcHABRScgjGh1NlZWU0c+ZMmjVrFn366afU09NDy5YtU1EdAIilAHr77bfpzz//pMrKSrLb7TRjxgw6duzYfR3TABDdlJwHFIoOruTkZNkZjT4gAP1+RnEtGAAogwACAGUQQACgDAIIAJRBAAGAMgggAFAGAQQAyiCAAEAZBBAAKIMAAgBlEEAAoAwCCACUQQABgDIIIABQBgEEAMoggABAGQQQACiDAAIAZRBAAKAMAggAlEEAAYAyCCAAUAYBBADKIIAAQBkEEAAogwACAGUQQACgDAIIAPQTQCdPnqQ33niDsrKyyGAw0Pfffx+0XQhBlZWV9NRTT1FCQgKVlJTQr7/+GlTm1q1btGTJEnnT+pSUFFq+fDnduXNn5K8GAKI7gHp6emj69Om0Z8+eQbfv3LmTdu/eTV9++SX9/PPPNHr0aCotLaXe3l6tDIdPW1sb1dfXU11dnQy1lStXjuyVAID+iBHgh9fW1mrLfr9fZGZmik8++URb53Q6hcViEd98841cvnjxonzcmTNntDJHjx4VBoNBXL16dUg/1+VyyefgOQBEnqF+RkPaB/T777+T3W6Xh10BycnJVFRURDabTS7znA+7Zs6cqZXh8kajUbaYBuN2u6mrqytoAgD9C2kAcfiwjIyMoPW8HNjG8/T09KDtcXFxlJqaqpUZqKqqSgZZYMrJyQlltQFAEV2MglVUVJDL5dKmjo4O1VUCgEgLoMzMTDnv7OwMWs/LgW08dzgcQdu9Xq8cGQuUGchiscgRs/4TAOhfSANo4sSJMkQaGhq0ddxfw307xcXFcpnnTqeTWlpatDKNjY3k9/tlXxEAxI644T6Az9f57bffgjqeW1tbZR9Obm4urVu3jrZv306TJk2SgbR582Z5ztDChQtl+WeffZZee+01WrFihRyq7+vro9WrV9M777wjywFADBnu8Nrx48fl8NrAqaysTBuK37x5s8jIyJDD73PnzhXt7e1Bz3Hz5k3x7rvviqSkJGG1WsWyZctEd3d3yIf4AECNoX5GDfwP6Qwf1vFoGHdIoz8IQL+fUV2MggFAdEIAAYAyCCAAUAYBBADKIIAAQBkEEAAogwACAGUQQACgDAIIAJRBAAGAMgggAFAGAQQAyiCAAEAZBBAAKIMAAgBlEEAAoAwCCACUQQABgDIIIABQBgEEAPq5LQ9AuN292UH3bl3TluMSrGSdMJkMBoPSesHIIYAg4t363xa6/ssRbTkp82myZv0zkcGktF4wcjgEA93x+/rI7/eprgaEAAIIdEf4vCQQQFEBAQQRz2AK7inw97lJePuU1QdCBwEEES8xdQKR4a+3queuk7zuO0rrBKGBAIKIZzSPCh7xkncdV1kjCBUEEEQ8k9miugoQCQFUVVVFhYWFNGbMGEpPT6eFCxdSe3t7UJne3l4qLy+ntLQ0SkpKosWLF1NnZ2dQmStXrtCCBQsoMTFRPs/GjRvJ6/WG5hVB1DHGxXNPkOpqgOoAampqkuFy+vRpqq+vp76+Ppo3bx719PRoZdavX0+HDx+mmpoaWf7atWu0aNEibbvP55Ph4/F46NSpU3TgwAHav38/VVZWhvaVQdQwGE0D8keQ34c/WFFBjIDD4ZBH401NTXLZ6XQKs9ksampqtDKXLl2SZWw2m1w+cuSIMBqNwm63a2Wqq6uF1WoVbrd7SD/X5XLJ5+Q5RL+7t66JM/+xSjR/uUJOZ/72gei69j+qqwUh+IyOqA/I5XLJeWpqqpy3tLTIVlFJSYlWZvLkyZSbm0s2m00u83zq1KmUkZGhlSktLaWuri5qa2sb9Oe43W65vf8EsUsIQb4+t+pqQAg8dgD5/X5at24dzZkzh6ZMmSLX2e12io+Pp5SUlKCyHDa8LVCmf/gEtge2PajvKTk5WZtycnIet9qgR3IELLgPyN/Xq6w6EAEBxH1BFy5coEOHDtGTVlFRIVtbgamjo+OJ/0yIHHGWJIpP+nsrWxJ+unvrqsoqgcqLUVevXk11dXV08uRJys7O1tZnZmbKzmWn0xnUCuJRMN4WKNPc3Bz0fIFRskCZgSwWi5wgNhlNcWSSI2F/waUYMdgC4mNvDp/a2lpqbGykiRMnBm0vKCggs9lMDQ0N2joepudh9+LiYrnM8/Pnz5PD4dDK8Iia1Wql/Pz8kb8iiMpRMIPJrLoaoLoFxIddBw8epB9++EGeCxTos+F+mYSEBDlfvnw5bdiwQXZMc6isWbNGhs7s2bNlWR6256BZunQp7dy5Uz7Hpk2b5HOjlQMPCiBuBQ32BxHfCRRDAVRdXS3nL7/8ctD6ffv20fvvvy//v2vXLjIajfIERB694hGuvXv3amVNJpM8fFu1apUMptGjR1NZWRlt27YtNK8Iog+HjDG4sY5DsOhg4LF40hkehufWFndIcysLohu/Rdv/cxd1X72srRv/7L/SP/zLv5Gh30WqoL/PKH57oEt+r4eTSXU1YIQQQKALA3t6+EREHTbeYQAEEOhCAn8nUD+9Tjv6gaIAAgh0ge+E0Z/f58GXAkUBBBDoAr4TKDohgEAXjAigqIQAAl0wGgecsiYECb9fVXUgRBBAEPEGO9uZw4fvDwb6hgACXRLCRwIBpHsIINDP17L2I3w+fClZFEAAgS5YrOPl7XkCfJ675Om+obROMHIIINDNnTEMAy9IxZnQuocAAl0wxplx4WkUwm8UdMFoig+6PTNEB/xGQRcMJtN9w/F8LRgOw/QNAQS65ccomO4hgEC3MAyvfwgg0I8BfUA+zz1lVYHQQACBLhhNZhqVnB607u5N3B9O7xBAoA8GI5niE4JWCZ9XWXUgNBBAoAs8AsYnI0J0QQCBPhiMZIzDdwJFGwQQ6MbAmxMKge8D0jsEEOj3O4F8XnwxfSzdGRXgSeKzmvmGdg86u5nvtNufx32PXM7bD7xvPN+hl28hjts3Ry4EEEQMDp+5c+eS3W4fdPu/l+bT64V52nLb+f+mRRsL6Z5n8NGwZ555ho4ePUrx8ei8jlQIIIgY3PLh8Ll69eqg2y/8ZqXiGbPo994ZZCQfjUs4RV3OG+S43TNo+ZSUFFwrFk19QNXV1TRt2jR5r2eeiouL5V+YgN7eXiovL6e0tDRKSkqixYsXU2dnZ9BzXLlyhRYsWECJiYmUnp5OGzduJK8X53PAo111memXrrnk8OSR3fNPdKl3HvkpUXW1IFwBlJ2dTTt27KCWlhY6e/YsvfLKK/Tmm29SW1ub3L5+/Xo6fPgw1dTUUFNTE127do0WLVqkPd7n88nw8Xg8dOrUKTpw4ADt37+fKisrR/IaIEZ03TNSr/+vwLnrG0NeMXj/D+iEGKGxY8eKr776SjidTmE2m0VNTY227dKlS9z+FTabTS4fOXJEGI1GYbfbtTLV1dXCarUKt9s95J/pcrnk8/Icosft27fFhAkT5O92sCn/H3PF51Xfia3bfxbbtv8sPtt+QGSkjn1g+eeee0709vaqflkxyTXEz+hj9wFxa4ZbOj09PfJQjFtFfX19VFJSopWZPHky5ebmks1mo9mzZ8v51KlTKSMjQytTWlpKq1atkq2o559/flh1uHz5sjzUg+jQ3d0t30MP0nnjT/qvxp3k8Ewko8FHaab/o557g/f/BEbN+D1iNqOVFG537twZUrlhB9D58+dl4HB/D3/4a2trKT8/n1pbW+VoA3f89cdhExjV4Hn/8AlsD2x72Bup/xAsj5Ywl8uF/qMoe9M+rNP4Ztc9OlTfTEQ8PZrf75fvkbg4jLWEGzdMhmLYvxke2uSw4V/sd999R2VlZbK/50mqqqqirVu33re+qKhIdoZDdHA6nSEdMk9ISJDvEYsFl3CEW6CREPIzofkN8vTTT1NBQYEMhunTp9Nnn31GmZmZsnOZ30T98SgYb2M8HzgqFlgOlBlMRUWFDLzA1NGBr2EAiAYjvhSDm7l8eMSBxMfaDQ0N2rb29nY57M6HbIznfAjncDi0MvX19bIVw4dxD8J/wQJD/4EJAPRvWIdg3BKZP3++7FjmDsODBw/SiRMn6Mcff6Tk5GRavnw5bdiwgVJTU2VIrFmzRoYOd0CzefPmyaBZunQp7dy5U/b7bNq0SZ47hGYyQOwZVgBxy+W9996j69evy8DhkxI5fF599VW5fdeuXfL6Gz4BkVtFPMK1d+9e7fEmk4nq6urkqBcH0+jRo2Uf0rZt20L/ykB3+JotvnYrVC1cHiTBdWCRzcBj8aTDDi4OQO4PwuFY9ODDeW4V8ykeocBdAjzKihCK3M8oxichYnDrOSsrS3U1IIzwfUAAoAwCCACUQQABgDIIIABQBgEEAMoggABAGQQQACiDAAIAZRBAAKAMAggAlEEAAYAyCCAAUAYBBADKIIAAQBkEEAAogwACAGUQQACgDAIIAJRBAAGAMgggAFAGAQQAyiCAAEAZBBAAKIMAAgBlEEAAoAwCCACUQQABgDIIIABQBgEEAMoggABAmTjSISGEnHd1damuCgAMIvDZDHxWoyqAbt68Kec5OTmqqwIAD9Hd3U3JycnRFUCpqalyfuXKlYe+OLj/rxKHdkdHB1mtVtXV0QXss8fDLR8On6ysrIeW02UAGY1/77ri8MGbYvh4n2G/DQ/22fANpXGATmgAUAYBBADK6DKALBYLbdmyRc5h6LDfhg/77MkyiEeNkwEAPCG6bAEBQHRAAAGAMgggAFAGAQQAyugygPbs2UN5eXk0atQoKioqoubmZopVVVVVVFhYSGPGjKH09HRauHAhtbe3B5Xp7e2l8vJySktLo6SkJFq8eDF1dnYGleGzyhcsWECJiYnyeTZu3Eher5diwY4dO8hgMNC6deu0ddhnYSJ05tChQyI+Pl58/fXXoq2tTaxYsUKkpKSIzs5OEYtKS0vFvn37xIULF0Rra6t4/fXXRW5urrhz545W5oMPPhA5OTmioaFBnD17VsyePVu8+OKL2nav1yumTJkiSkpKxC+//CKOHDkixo0bJyoqKkS0a25uFnl5eWLatGli7dq12nrss/DQXQDNmjVLlJeXa8s+n09kZWWJqqoqpfWKFA6Hg0+rEE1NTXLZ6XQKs9ksampqtDKXLl2SZWw2m1zmD4/RaBR2u10rU11dLaxWq3C73SJadXd3i0mTJon6+nrx0ksvaQGEfRY+ujoE83g81NLSQiUlJUHXhfGyzWZTWrdI4XK5gi7Y5f3V19cXtM8mT55Mubm52j7j+dSpUykjI0MrU1paKi/EbGtro2jFh1h8CNV/3zDss/DR1cWoN27cIJ/PF/RLZ7x8+fJlinV+v1/2Y8yZM4emTJki19ntdoqPj6eUlJT79hlvC5QZbJ8GtkWjQ4cO0blz5+jMmTP3bcM+Cx9dBRA8+i/6hQsX6KefflJdlYjGX62xdu1aqq+vlwMZoI6uDsHGjRtHJpPpvtEIXs7MzKRYtnr1aqqrq6Pjx49Tdna2tp73Cx+6Op3OB+4zng+2TwPbog0fYjkcDnrhhRcoLi5OTk1NTbR79275f27JYJ+Fh64CiJvFBQUF1NDQEHTYwcvFxcUUi3gggcOntraWGhsbaeLEiUHbeX+ZzeagfcbD9DyEHNhnPD9//rz8UAZw64C//yY/P5+izdy5c+XrbW1t1aaZM2fSkiVLtP9jn4WJ0OEwvMViEfv37xcXL14UK1eulMPw/UcjYsmqVatEcnKyOHHihLh+/bo23b17N2hImYfmGxsb5ZBycXGxnAYOKc+bN08O5R87dkyMHz8+poaU+4+CMeyz8NBdALHPP/9cvjn4fCAelj99+rSIVfw3ZLCJzw0KuHfvnvjwww/F2LFjRWJionjrrbdkSPX3xx9/iPnz54uEhAR5PstHH30k+vr6RKwGEPZZeODrOABAGV31AQFAdEEAAYAyCCAAUAYBBADKIIAAQBkEEAAogwACAGUQQACgDAIIAJRBAAGAMgggAFAGAQQApMr/AzrD3WoVI22zAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(True)[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop (3.9.23)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
