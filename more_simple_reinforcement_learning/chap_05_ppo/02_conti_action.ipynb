{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6bda73c",
   "metadata": {},
   "source": [
    "基本原理和离散动作是一样的,连续动作的概率使用高斯密度函数计算即可."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2df567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "#定义环境\n",
    "class MyWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self):\n",
    "        env = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.step_n = 0\n",
    "\n",
    "    def reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        self.step_n = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, terminated, truncated, info = self.env.step(\n",
    "            [action * 2])\n",
    "        over = terminated or truncated\n",
    "\n",
    "        #偏移reward,便于训练\n",
    "        reward = (reward + 8) / 8\n",
    "\n",
    "        #限制最大步数\n",
    "        self.step_n += 1\n",
    "        if self.step_n >= 200:\n",
    "            over = True\n",
    "\n",
    "        return state, reward, over\n",
    "\n",
    "    #打印游戏图像\n",
    "    def show(self):\n",
    "        from matplotlib import pyplot as plt\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(self.env.render())\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()\n",
    "\n",
    "env.show()\n",
    "\n",
    "\n",
    "'''\n",
    "高中生版本解說\n",
    "\n",
    "  我們換了一個新遊戲，叫做「倒立擺 (Pendulum)」。\n",
    "\n",
    "   * 遊戲目標：想像一根掛在時鐘中心的指針，它會自然下垂。你的目標是施加一個力（力矩），讓這根指針能反抗地心引力，成功地向上豎起來，並保持直立\n",
    "     。\n",
    "   * 連續動作：你施加的力，不像之前只能「向左」或「向右」，而是可以施加任意大小的力（比如 +0.5, -1.8 等），這就是「連續動作」。\n",
    "\n",
    "  這個 MyWrapper 同樣修改了遊戲規則，主要有兩點不同：\n",
    "\n",
    "   1. 動作轉換器：我們的演員大腦，它習慣輸出 -1 到 +1 之間的力。但這個遊戲引擎，它需要接收 -2 到 +2 之間的力。所以 step 函數裡加了一句 [action\n",
    "      * 2]，就像一個「變速箱」，自動把演員的輸出乘以 2，來滿足遊戲引擎的要求。\n",
    "\n",
    "   2. 分數標準化：這個遊戲原始的計分方式很奇怪，分數都是像 -13.2、-5.7 這樣的負數。為了讓我們的評論家大腦更容易學習和判斷，reward = (reward +\n",
    "      8) / 8 這行程式碼把這些奇怪的分數，全部轉換到 -1 到 +1\n",
    "      這個比較「標準」的區間內。這就像把華氏溫度轉換成攝氏溫度，數字更規整，更容易比較和學習。\n",
    "\n",
    "  專業術語解說\n",
    "\n",
    "  這個 Cell 為 Pendulum-v1 環境定義了一個新的 Wrapper，主要進行了動作空間映射和獎勵歸一化。\n",
    "\n",
    "   1. 環境 (`Pendulum-v1`):\n",
    "       * 這是一個經典的連續控制任務。\n",
    "       * 狀態空間 (State Space)：3 維，包含 [cos(theta), sin(theta), a_velocity]，即角度的餘弦、正弦和角速度。\n",
    "       * 動作空間 (Action Space)：1 維，代表施加的力矩 (torque)，其有效範圍是 [-2.0, 2.0]。\n",
    "\n",
    "   2. 動作空間映射 (Action Space Mapping):\n",
    "       * self.env.step([action * 2]) 這一行至關重要。它解決了策略網路的輸出範圍和環境要求的動作範圍不匹配的問題。\n",
    "       * 在連續控制中，策略網路的輸出層通常會使用 tanh 激活函數，它的值域是 [-1, 1]。這樣做可以避免網路輸出的值無限大，讓訓練更穩定。\n",
    "       * 因此，我們需要將 tanh 輸出的 [-1, 1] 範圍的動作，線性地映射到環境所需的 [-2, 2] 範圍。這裡的 * 2 就是在執行這個映射。\n",
    "\n",
    "   3. 獎勵歸一化 (Reward Normalization):\n",
    "       * reward = (reward + 8) / 8 是一種常見且有效的獎勵塑造 (Reward Shaping) 技術。\n",
    "       * Pendulum-v1 的原始獎勵是一個與角度和角速度相關的代價函數 (cost)，其範圍大致在 [-16.27,\n",
    "         0]。直接使用這個範圍的獎勵，可能會因為其數值過大且分佈不均，導致價值網路的學習不穩定。\n",
    "       * 通過 + 8 和 / 8，我們將獎勵信號歸一化到一個更穩定、更對稱的範圍（約 [-1, 1]）。這有助於穩定 Critic\n",
    "         的訓練，防止梯度爆炸或消失，並可能加速學習過程。\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "，獎勵歸一化確實是為了穩定價值網路 (Critic) 的訓練，而穩定的價值網路，是整個 PPO 演算法能夠成功收斂的基石。\n",
    "\n",
    "  讓我來解釋這背後的因果關係。\n",
    "\n",
    "  ---\n",
    "\n",
    "  梯度從何而來？\n",
    "\n",
    "  首先，我們要記住，梯度爆炸或消失，問題出在價值網路 (`model_value`) 的訓練過程中。\n",
    "\n",
    "  價值網路的學習目標，是讓自己的預測 value 盡可能地接近「標準答案」target。它的損失函數是：\n",
    "  Loss = (value - target)²\n",
    "\n",
    "  在反向傳播時，第一步計算的梯度，就是這個 Loss 對 value 的導數，也就是：\n",
    "  Gradient ≈ 2 * (value - target)\n",
    "\n",
    "  這就是關鍵點：梯度的初始大小，直接取決於 (value - target) 這個差值的大小。\n",
    "\n",
    "  ---\n",
    "\n",
    "  兩種情境對比\n",
    "\n",
    "  讓我們用倒立擺的例子，來對比一下「歸一化」和「不歸一化」的差別。\n",
    "\n",
    "  情境一：不使用獎勵歸一化 (有風險)\n",
    "\n",
    "   * 獎勵範圍：倒立擺的原始獎勵 reward 範圍約為 [-16, 0]。\n",
    "   * Target 的範圍：target 的計算公式是 reward + 0.98 * V(s_next)。這意味著 target 的值也會是很大的負數，比如 -15, -12 等。\n",
    "   * 訓練初期：在訓練剛開始時，你的價值網路 model_value 還是一個「菜鳥」，它的權重是隨機的，所以它的輸出 value 可能是一個接近 0 的小數，比如\n",
    "     0.5。\n",
    "   * 梯度計算：\n",
    "       * value (預測值) = 0.5\n",
    "       * target (標準答案) = -15\n",
    "       * Gradient ≈ 2 * (0.5 - (-15)) = 31\n",
    "   * 問題所在：31 是一個非常大的梯度值！ 當這個巨大的梯度從輸出層開始，通過鏈式法則反向傳播回網路的每一層時，它會被逐層放大（或縮小）。這極有\n",
    "     可能導致網路底層的權重發生劇烈的、爆炸性的更新。\n",
    "   * 結果：網路的權重可能會瞬間變成 inf (無窮大) 或 NaN (非數)，整個模型直接「炸掉」，訓練失敗。這就是梯度爆炸。\n",
    "\n",
    "  情境二：使用獎勵歸一化 (更穩定)\n",
    "\n",
    "   * 獎勵範圍：通過 (reward + 8) / 8，我們把獎勵 reward 的範圍縮放到 [-1, 1] 之間。\n",
    "   * Target 的範圍：現在，target 的值也會被限制在一個很小的、可預測的範圍內。\n",
    "   * 訓練初期：model_value 的輸出 value 仍然是 0.5。\n",
    "   * 梯度計算：\n",
    "       * 原始 reward = -15，歸一化後 reward = (-15 + 8) / 8 = -0.875。\n",
    "       * value (預測值) = 0.5\n",
    "       * target (標準答案) ≈ -0.875\n",
    "       * Gradient ≈ 2 * (0.5 - (-0.875)) = 2.75\n",
    "   * 優勢：2.75 是一個溫和得多的梯度值。當這個小而穩定的梯度回傳時，網路的權重只會進行平滑的、微小的調整。\n",
    "   * 結果：訓練過程非常穩定，網路的權重會逐步地、可控地向著正確的方向收斂，有效避免了梯度爆炸或消失的風險。\n",
    "\n",
    "  總結\n",
    "\n",
    "  可以把獎勵歸一化，看作是為我們的價值網路（評論家）提供了一套「溫和的、標準化的課程」。\n",
    "\n",
    "   * 不歸一化，就像是讓一個小學生直接去解大學微積分，題目太難（target 值太大），他要麼當場崩潰（梯度爆炸），要麼完全放棄（梯度消失）。\n",
    "   * 歸一化，則是把大學微積分的知識，拆解成從「加減乘除」到「代數」再到「極限」的標準化課程。學生每一步學到的知識都在可控範圍內，他的能力（網\n",
    "     路權重）因此能夠穩定、持續地成長。\n",
    "\n",
    "  所以，獎勵歸一化通過約束 Critic 損失函數中目標值的範圍，間接地控制了梯度的初始大小，從而保護了整個神經網路在反向傳播過程中免受劇烈衝擊，這\n",
    "  是保證深度強化學習訓練穩定性的關鍵一步。\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "967d2307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[0.0291],\n",
       "          [0.0673]], grad_fn=<TanhBackward0>),\n",
       "  tensor([[0.8901],\n",
       "          [0.8493]], grad_fn=<ExpBackward0>)),\n",
       " tensor([[ 0.0453],\n",
       "         [-0.1344]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "#定义模型\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.s = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.mu = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64, 1),\n",
    "            torch.nn.Tanh(),\n",
    "        )\n",
    "        self.sigma = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64, 1),\n",
    "            torch.nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = self.s(state)\n",
    "\n",
    "        return self.mu(state), self.sigma(state).exp()\n",
    "\n",
    "\n",
    "model_action = Model()\n",
    "\n",
    "model_value = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 1),\n",
    ")\n",
    "\n",
    "model_action(torch.randn(2, 3)), model_value(torch.randn(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b511423",
   "metadata": {},
   "source": [
    "\n",
    "  平衡車 (CartPole-v1) 的 4 個數字\n",
    "\n",
    "  平衡車的狀態是一個包含 4 個數字的列表，它們共同描述了「車」和「杆」的瞬時情況：\n",
    "\n",
    "   1. 車的位置 (Cart Position)：\n",
    "       * 意義：小車在水平軌道上的位置。\n",
    "       * 數值：0.0 代表軌道正中央。正數代表在右邊，負數代表在左邊。\n",
    "\n",
    "   2. 車的速度 (Cart Velocity)：\n",
    "       * 意義：小車的移動速度和方向。\n",
    "       * 數值：正數代表向右移動，負數代表向左移動。\n",
    "\n",
    "   3. 杆的角度 (Pole Angle)：\n",
    "       * 意義：杆子偏離垂直線的角度。\n",
    "       * 數值：0.0 代表完美垂直。正數代表向右傾斜，負數代表向左傾斜。\n",
    "\n",
    "   4. 杆的角速度 (Pole Angular Velocity)：\n",
    "       * 意義：杆子頂端的摔倒速度。\n",
    "       * 數值：描述了杆子角度的變化率。如果杆子正在向右倒，這個值就是正的。\n",
    "\n",
    "  所以，演員大腦就是根據這四個數字（車的位置、車速、杆的角度、杆的角速度）來決定下一步是向左還是向右推小車。\n",
    "\n",
    "  ---\n",
    "\n",
    "  倒立擺 (Pendulum-v1) 的 3 個數字\n",
    "\n",
    "  倒立擺的狀態稍微有點不同，它用 3 個數字來描述：\n",
    "\n",
    "   1. 角度的餘弦 `cos(theta)`\n",
    "   2. 角度的正弦 `sin(theta)`\n",
    "   3. 角速度 (Angular Velocity)\n",
    "\n",
    "  你可能會問：為什麼不直接用角度 `theta`，而是要用 `cos(theta)` 和 `sin(theta)` 這麼麻煩？\n",
    "\n",
    "  這是一個非常聰明的設計，為了解決一個叫做「不連續性」的問題。\n",
    "\n",
    "   * 問題所在：角度是有循環的。359 度和 1 度在物理上幾乎是同一個位置，但 359 和 1\n",
    "     這兩個數字本身卻相差很遠。如果直接把角度數字餵給神經網路，它很難理解這種「循環」的特性，可能會在 0 度附近做出非常奇怪的決策。\n",
    "   * 解決方案：通過 cos(theta) 和 sin(theta)，我們可以把一個一維的角度，完美地轉換成一個二維圓上的點 (x, y)。當角度從 359 度平滑地過渡到 1\n",
    "     度時，這個點也會在圓上平滑地移動。這樣，神經網路就能更好地理解角度的連續性和週期性，做出更合理的判斷。\n",
    "\n",
    "  最後一個數字角速度 (Angular Velocity) 和平衡車的杆的角速度意義一樣，代表了擺杆正在旋轉的速度和方向。\n",
    "\n",
    "  所以，在倒立擺遊戲中，演員大腦是根據 (cos(theta), sin(theta), 角速度) 這三個數字，來決定要施加多大的力矩。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d5fdc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\it_project\\github_sync\\ml-workshop\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_44956\\3994207745.py:34: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  state = torch.FloatTensor(state).reshape(-1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30.417152404785156"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "import random\n",
    "\n",
    "\n",
    "#玩一局游戏并记录数据\n",
    "def play(show=False):\n",
    "    state = []\n",
    "    action = []\n",
    "    reward = []\n",
    "    next_state = []\n",
    "    over = []\n",
    "\n",
    "    s = env.reset()\n",
    "    o = False\n",
    "    while not o:\n",
    "        #根据概率采样\n",
    "        mu, sigma = model_action(torch.FloatTensor(s).reshape(1, 3))\n",
    "        a = random.normalvariate(mu=mu.item(), sigma=sigma.item())\n",
    "\n",
    "        ns, r, o = env.step(a)\n",
    "\n",
    "        state.append(s)\n",
    "        action.append(a)\n",
    "        reward.append(r)\n",
    "        next_state.append(ns)\n",
    "        over.append(o)\n",
    "\n",
    "        s = ns\n",
    "\n",
    "        if show:\n",
    "            display.clear_output(wait=True)\n",
    "            env.show()\n",
    "\n",
    "    state = torch.FloatTensor(state).reshape(-1, 3)\n",
    "    action = torch.FloatTensor(action).reshape(-1, 1)\n",
    "    reward = torch.FloatTensor(reward).reshape(-1, 1)\n",
    "    next_state = torch.FloatTensor(next_state).reshape(-1, 3)\n",
    "    over = torch.LongTensor(over).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over, reward.sum().item()\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over, reward_sum = play()\n",
    "\n",
    "reward_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de97706",
   "metadata": {},
   "source": [
    " 高中生版本解說\n",
    "\n",
    "  這個新版的「遊戲記錄員」play 函數，主要有以下幾點變化：\n",
    "\n",
    "   1. 獲取行動指南：記錄員不再從演員那裡拿到一個像 [0.7, 0.3]\n",
    "      的機率列表，而是拿到一個更高級的「行動指南」——一個高斯分佈（鐘形曲線），這個分佈由「最佳猜測 mu」和「不確定性 sigma」共同定義。\n",
    "\n",
    "   2. 選擇具體動作：記錄員使用 random.normalvariate() 這個工具，在這個鐘形曲線上隨機抽取一個點。這個點就是一個具體的、連續的數值（比如\n",
    "      -0.85、1.23），代表了這一步要施加的力的大小。這就像在一個靶子上射擊，mu 是靶心，sigma 決定了你的射擊散佈範圍，而 a\n",
    "      就是你這一槍實際打中的位置。\n",
    "\n",
    "   3. 記錄浮點數動作：因為動作不再是 0 或 1，而是一個浮點數，所以在最後整理數據時，action 列表被轉換成 FloatTensor。\n",
    "\n",
    "  其他部分，如記錄狀態、獎勵、是否結束等，都和之前完全一樣。\n",
    "\n",
    "  專業術語解說\n",
    "\n",
    "  這個 play 函數的核心改動在於策略的執行和動作的採樣。\n",
    "\n",
    "   1. 策略參數化: mu, sigma = model_action(...) 這一行執行了策略網路，獲取了定義高斯策略 $\\mathcal{N}(\\mu, \\sigma)$ 的兩個參數。\n",
    "\n",
    "   2. 動作採樣: a = random.normalvariate(...) 從上述定義的高斯分佈中採樣一個連續動作值 a。這是執行隨機高斯策略的具體實現。\n",
    "\n",
    "   3. 數據類型: 由於動作 a 是連續的浮點數，因此在函數末尾，action 列表被轉換為 torch.FloatTensor，以匹配其數據類型。\n",
    "\n",
    "  一個關於梯度的重要說明\n",
    "\n",
    "  你可能會注意到，這裡使用的是 Python 內建的 random.normalvariate，而不是我們之前討論過的、可微分的 dist.rsample()。這是一個非常關鍵的區別：\n",
    "\n",
    "   * 在 `play` 函數中，這樣做是完全可以的。因為 play\n",
    "     函數的唯一目的是收集數據。在這個階段，我們只是在執行策略，並不需要計算梯度和進行反向傳播。我們只需要一個具體的動作 a 來和環境互動。\n",
    "   * 梯度在哪裡？梯度的問題會在 train_action 函數中解決。在訓練時，我們會根據儲存的狀態 s，重新計算一次 mu 和 sigma，然後用 PyTorch 的\n",
    "     distributions.Normal 來創建一個可微分的分佈，再用這個分佈去計算我們儲存的動作 a 的對數機率 log_prob(a)。\n",
    "\n",
    "  這種將「執行採樣」和「訓練時的機率計算」分離開來的做法，在實作上是完全可行的，並且能正常工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a22c5ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_action = torch.optim.Adam(model_action.parameters(), lr=5e-4)\n",
    "optimizer_value = torch.optim.Adam(model_value.parameters(), lr=5e-3)\n",
    "\n",
    "\n",
    "def requires_grad(model, value):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f13610",
   "metadata": {},
   "source": [
    "  高中生版本解說\n",
    "\n",
    "  這個 cell 依然是在為我們的兩個大腦（演員和評論家）聘請「專屬教練 (`Optimizer`)」。\n",
    "\n",
    "   * 功能不變：和上次一樣，我們還是聘請了 Adam 這位全能教練，並且也準備了 requires_grad\n",
    "     這個「訓練開關」，用來在訓練時凍結其中一個大腦。整個工作的基本模式沒有任何變化。\n",
    "\n",
    "   * 唯一的調整：學習率 (`lr`)：\n",
    "       * 這次，我們給兩位教練的「學習率」都調低了。\n",
    "           * 演員的學習率從 0.001 降到了 0.0005。\n",
    "           * 評論家的學習率從 0.01 降到了 0.005。\n",
    "       * 為什麼要調低？ 因為「倒立擺」這個新遊戲比「平衡車」更精細、更複雜。我們可以把它想像成，從學「走路」變成了學「走鋼絲」。在這種更需要技\n",
    "         巧和平衡的任務中，我們需要讓大腦的學習過程更謹慎、更平滑。較低的學習率，就意味著教練每次只會對大腦進行更微小的調整，一步一個腳印，避\n",
    "         免因為某次調整過大而「從鋼絲上掉下來」（即策略崩潰）。\n",
    "\n",
    "  專業術語解說\n",
    "\n",
    "  此 cell 的功能與離散動作版本中的對應 cell 完全相同：初始化 Actor 和 Critic 的 Adam 優化器，並定義 requires_grad 輔助函數。\n",
    "\n",
    "  這展示了 PPO 演算法中不同模塊的獨立性——無論策略的具體形式如何（離散或連續），優化器的設置和梯度控制的邏輯都可以保持不變。\n",
    "\n",
    "  唯一的、但也是很重要的改動是超參數（Hyperparameter）的調整：\n",
    "\n",
    "   * 學習率 (Learning Rates)：兩個網路的學習率都被調低了（演員：1e-3 -> 5e-4；評論家：1e-2 -> 5e-3）。\n",
    "   * 調整原因：連續控制問題的策略空間通常比離散問題更複雜、更敏感。較低的學習率有助於增強訓練的穩定性。它使得權重更新的步長變小，減少了因某次\n",
    "     梯度更新過大而導致策略性能急劇下降的風險，讓策略能夠在複雜的「損失地形」上進行更平穩的搜索。\n",
    "   * 學習率比例：儘管絕對值降低了，但評論家的學習率仍然是演員的 10 倍，這保留了讓價值函數比策略更快收斂的設計思想。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9657c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_value(state, reward, next_state, over):\n",
    "    requires_grad(model_action, False)\n",
    "    requires_grad(model_value, True)\n",
    "\n",
    "    #计算target\n",
    "    with torch.no_grad():\n",
    "        target = model_value(next_state)\n",
    "    target = target * 0.98 * (1 - over) + reward\n",
    "\n",
    "    #每批数据反复训练10次\n",
    "    for _ in range(10):\n",
    "        #计算value\n",
    "        value = model_value(state)\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(value, target)\n",
    "        loss.backward()\n",
    "        optimizer_value.step()\n",
    "        optimizer_value.zero_grad()\n",
    "\n",
    "    #减去value相当于去基线\n",
    "    return (target - value).detach()\n",
    "\n",
    "\n",
    "value = train_value(state, reward, next_state, over)\n",
    "\n",
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b92ff9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01315974723547697"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_action(state, action, value):\n",
    "    requires_grad(model_action, True)\n",
    "    requires_grad(model_value, False)\n",
    "\n",
    "    #计算优势函数\n",
    "    delta = []\n",
    "    for i in range(len(value)):\n",
    "        s = 0\n",
    "        for j in range(i, len(value)):\n",
    "            s += value[j] * (0.9 * 0.9)**(j - i)\n",
    "        delta.append(s)\n",
    "    delta = torch.FloatTensor(delta).reshape(-1, 1)\n",
    "\n",
    "    #更新前的动作概率\n",
    "    with torch.no_grad():\n",
    "        mu, sigma = model_action(state)\n",
    "        prob_old = torch.distributions.Normal(mu, sigma).log_prob(action).exp()\n",
    "\n",
    "    #每批数据反复训练10次\n",
    "    for _ in range(10):\n",
    "        #更新后的动作概率\n",
    "        mu, sigma = model_action(state)\n",
    "        prob_new = torch.distributions.Normal(mu, sigma).log_prob(action).exp()\n",
    "\n",
    "        #求出概率的变化\n",
    "        ratio = prob_new / prob_old\n",
    "\n",
    "        #计算截断的和不截断的两份loss,取其中小的\n",
    "        surr1 = ratio * delta\n",
    "        surr2 = ratio.clamp(0.8, 1.2) * delta\n",
    "\n",
    "        loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        #更新参数\n",
    "        loss.backward()\n",
    "        optimizer_action.step()\n",
    "        optimizer_action.zero_grad()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "train_action(state, action, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b75b2ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.24221140146255493 28.25833005607128\n",
      "100 0.10609608888626099 72.09614181518555\n",
      "200 0.159645676612854 41.67578239440918\n",
      "300 -0.5503265857696533 165.66928825378417\n",
      "400 -0.632102370262146 157.85325393676757\n",
      "500 -0.8215871453285217 162.35944175720215\n",
      "600 0.06191960349678993 164.19886589050293\n",
      "700 -1.5193116664886475 135.93767490386963\n",
      "800 -0.41928574442863464 108.27153968811035\n",
      "900 -0.043920379132032394 168.85371627807618\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model_action.train()\n",
    "    model_value.train()\n",
    "\n",
    "    #训练N局\n",
    "    for epoch in range(1000):\n",
    "        #一个epoch最少玩N步\n",
    "        steps = 0\n",
    "        while steps < 200:\n",
    "            state, action, reward, next_state, over, _ = play()\n",
    "            steps += len(state)\n",
    "\n",
    "            #训练两个模型\n",
    "            delta = train_value(state, reward, next_state, over)\n",
    "            loss = train_action(state, action, delta)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            test_result = sum([play()[-1] for _ in range(20)]) / 20\n",
    "            print(epoch, loss, test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82cdcc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR8AAAEXCAYAAACUBEAgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAYuklEQVR4nO3dC2yT573H8b+dewhOCJAERlLQoIUcLiuXQlapTCMjdFlXBpMYQiyjOa1KAXGZ0JoNUhVNCqLSWGkh3VStoEktU7pBByVto3BrRcolNBukkHVHtESkTrg0FzLi3J6j5zmzT8zSlkDix3a+H+nt69fvY/vvYP/6XBzHoZRSAgAB5gz0AwKARvgAsILwAWAF4QPACsIHgBWEDwArCB8AVhA+AKwgfABYQfgAGFzhs3PnThk7dqzExsbK7Nmz5dSpU7ZKATBYwudPf/qTbNiwQZ577jk5e/asTJs2TXJycqShocFGOQAscNj4xVLd05k1a5a8/PLL5ri7u1vS09NlzZo18uyzz37t7XX7uro6GTp0qDgcjgBUDOBO6DhpaWmR0aNHi9P51X2bSAmw9vZ2qayslIKCAt91usjs7GypqKjo9TYej8dsXleuXJHMzMyA1Aug72pra2XMmDHBFT7Xrl2Trq4uSU1N9bteH1+8eLHX2xQVFcnzzz/f6xN0uVwDViuAvmlubjajGD0q+ToBD5+7oXtJeo7o9ieog4fwAYLPnUyHBDx8RowYIREREVJfX+93vT5OS0vr9TYxMTFmAxA+Ar7aFR0dLTNmzJDy8nK/CWR9nJWVFehyAFhiZdilh1B5eXkyc+ZMeeihh+S3v/2ttLa2yooVK2yUA2CwhM+SJUvk6tWrUlhYKG63W771rW/JO++88x+T0ADCl5XP+dwrPeGcmJgoTU1NTDgDIfre5He7AFhB+ACwgvABYAXhA8AKwgeAFYQPACsIHwBWED4ArCB8AFhB+ACwgvABYAXhA8AKwgeAFYQPACsIHwBWED4ArCB8AFhB+ACwgvABYAXhA8AKwgeAFYQPACsIHwBWED4ArCB8AFhB+ACwgvABYAXhA8AKwgeAFYQPACsIHwBWED4ArCB8AFhB+ACwgvABEBrhc/z4cXnsscdk9OjR4nA4ZP/+/X7nlVJSWFgoo0aNkri4OMnOzpZPPvnEr82NGzdk2bJl4nK5JCkpSfLz8+XmzZv3/mwAhG/4tLa2yrRp02Tnzp29nt+2bZvs2LFDXnnlFTl58qQMGTJEcnJypK2tzddGB091dbWUlZXJwYMHTaA99dRT9/ZMAIQWdQ/0zfft2+c77u7uVmlpaeqFF17wXdfY2KhiYmLUG2+8YY4//vhjc7vTp0/72pSWliqHw6GuXLlyR4/b1NRk7kPvAQSPvrw3+3XO59KlS+J2u81QyysxMVFmz54tFRUV5ljv9VBr5syZvja6vdPpND2l3ng8HmlubvbbAIS2fg0fHTxaamqq3/X62HtO71NSUvzOR0ZGSnJysq/N7YqKikyIebf09PT+LBuABSGx2lVQUCBNTU2+rba21nZJAIIpfNLS0sy+vr7e73p97D2n9w0NDX7nOzs7zQqYt83tYmJizMpYzw1AaOvX8Bk3bpwJkPLyct91en5Gz+VkZWWZY71vbGyUyspKX5vDhw9Ld3e3mRsCMDhE9vUG+vM4//znP/0mmauqqsycTUZGhqxbt05+/etfy4QJE0wYbd682XwmaOHChab9pEmTZMGCBfLkk0+a5fiOjg5ZvXq1/OQnPzHtAAwSfV1KO3LkiFlKu33Ly8vzLbdv3rxZpaammiX2efPmqZqaGr/7uH79ulq6dKlKSEhQLpdLrVixQrW0tAzIch6AwOnLe9Oh/yMhRg/l9KqXnnxm/gcIzfdmSKx2AQg/hA8AKwgfAKGx2gX0Bz3V2NnUJJ76elEdHRLpcknMqFHiiIw035aA8Ef4IOC62trk2nvvybV33/WFT0RCgiRkZsqoJUsk/pvfJIAGAcIHAQ+euj/+Ua4eOiSqq+v/r29pkaaTJ+XWZ5/J2LVrTRARQOGNOR8EdKh14+hRuVpa6hc8PbW73VL7+99LZ2NjwOtDYBE+CJiumzel4e23RXV2fmW7W59+Kjfefz9gdcEOwgcB037tmunZfC2lxFNXF4iSYBHhg4BpPHFCuj2eO2uslBmmIXwRPgiYvoQJwRP+CB8EJ8In7BE+CN7wIYDCGuGDoKS6u22XgAFG+CA40esJe4QPgpKZcCaAwhrhg+BE8IQ9wgfBiTmfsEf4ICgx7Ap/hA+Ck/6Es+0aMKAIHwQnej1hj/BBUGLYFf4IHwQnJpzDHuGD4ESvJ+wRPghKDLvCH+GD4ETwhD3CB8GJOZ+wR/ggKDHsCn+ED4ITwRP2CB8Ebc+H+AlvhA+CE8OusEf4IDgx4Rz2CB8EJf56RfgjfBCcGHaFPcIHAeNwOO68McOusNen8CkqKpJZs2bJ0KFDJSUlRRYuXCg1NTV+bdra2mTVqlUyfPhwSUhIkMWLF0t9fb1fm8uXL0tubq7Ex8eb+9m4caN0fs3f70boi01PF0dk5B21baurk+62tgGvCSESPseOHTPB8uGHH0pZWZl0dHTI/PnzpbW11ddm/fr1cuDAASkpKTHt6+rqZNGiRb7zXV1dJnja29vlxIkTsmfPHtm9e7cUFhb27zND0HHGxuruzx217W5vZ94nzDnUPfwLX7161fRcdMg88sgj0tTUJCNHjpTXX39dfvzjH5s2Fy9elEmTJklFRYXMmTNHSktL5Qc/+IEJpdTUVNPmlVdekV/84hfm/qKjo7/2cZubmyUxMdE8nsvlutvyEWBNZ87I/xQViero+Nq2zvh4+a9duyQ6OTkgtaF/9OW9eU9zPvoBtOR/v0AqKytNbyg7O9vXZuLEiZKRkWHCR9P7KVOm+IJHy8nJMUVXV1f3+jgej8ec77khBPVlzgdh767Dp7u7W9atWycPP/ywTJ482VzndrtNzyUpKcmvrQ4afc7bpmfweM97z33ZXJNOU++Wnp5+t2XDJsIH/RE+eu7n/PnzsnfvXhloBQUFppfl3Wprawf8MdH/HE4WV/H/7mzp4TarV6+WgwcPyvHjx2XMmDG+69PS0sxEcmNjo1/vR6926XPeNqdOnfK7P+9qmLfN7WJiYsyGEEfPBz306X9Fem5aB8++ffvk8OHDMm7cOL/zM2bMkKioKCkvL/ddp5fi9dJ6VlaWOdb7c+fOSUNDg6+NXjnTk1OZmZl9KQeh+DkfAgh30/PRQy29kvXWW2+Zz/p452j0PExcXJzZ5+fny4YNG8wktA6UNWvWmMDRK12aXprXIbN8+XLZtm2buY9NmzaZ+6Z3E+YYduFuw6e4uNjsv/Od7/hd/9prr8nPfvYzc3n79u3idDrNhwv1KpVeydq1a5evbUREhBmyrVy50oTSkCFDJC8vT7Zs2dKXUhCK6PXgbsPnTj4SFBsbKzt37jTbl7nvvvvk0KFDfXlohAEmnNETrwYEDj0f9ED4IHCYcEYPhA8COuwieuBF+CBw6PWgB8IHwfl9Pgh7hA8Ch9Uu9MCrAYFDzwc9ED4IGH69Aj0RPggchl3ogVcDAoYJZ/RE+CBw6PmgB14NCBx6PrjXLxMD+mPCWf+isvvWLaltbZVhMTHyzaFDxUlADRqED6wMu3TwVDc2yvNVVfLZzZuSEBUl/33//bJk3DiJIIAGBYZdsDLhrL+c5aULF+TSzZui/zZpc0eHvHzhgpz/4gurNSJwCB8ETkSEOHt8W2XrbX+ltr27WzxdXRYKgw2EDwImKilJkh95xFzWfaDZI0b4DbHud7nkvoQEczkiJoYvHwtzzPkgYHSYjMjOluuHD0tnS4s8MWGCxEZEyIdXr8rI2Fh5ZuJESdF/UllEhs2dK5H8NdqwRvggoGIzMmTU0qVyZc8eiRcxk8z599/v+54fPS+UkJkpqQsX0vMJc4QPAkoHysgFC8zl+j//WTq++EIc//5ucEdUlLgefFDG5OfzN9oHAcIHAeeMipKU3FxJnD5dmj/6SDxutzjj4iRh0iSzRcTF2S4RAUD4wFoPKPYb3zAbBicG1QCsIHwAWEH4ALCC8AFgBeEDwArCB4AVhA8AKwgfAFYQPgCsIHwAWEH4ALCC8AFgBeEDwArCB0Dwh09xcbFMnTpVXC6X2bKysqS0tNR3vq2tTVatWiXDhw+XhIQEWbx4sdTX1/vdx+XLlyU3N1fi4+MlJSVFNm7cKJ23fZE4gPDXp/AZM2aMbN26VSorK+XMmTPy3e9+Vx5//HGprq4259evXy8HDhyQkpISOXbsmNTV1cmiRYt8t+/q6jLB097eLidOnJA9e/bI7t27pbCwsP+fGYDgpu7RsGHD1KuvvqoaGxtVVFSUKikp8Z27cOGC/n5MVVFRYY4PHTqknE6ncrvdvjbFxcXK5XIpj8fzpY/R1tammpqafFttba25X30ZQPDQ78k7fW/e9ZyP7sXs3btXWltbzfBL94Y6OjokOzvb12bixImSkZEhFRUV5ljvp0yZIqmpqb42OTk50tzc7Os99aaoqEgSExN9W3p6+t2WDSBI9Dl8zp07Z+ZzYmJi5Omnn5Z9+/ZJZmamuN1uiY6OlqSkJL/2Omj0OU3vewaP97z33JcpKCiQpqYm31ZbW9vXsgGE+nc4P/DAA1JVVWVC4M0335S8vDwzvzOQdNDpDcAgDh/duxk/fry5PGPGDDl9+rS8+OKLsmTJEjOR3NjY6Nf70atdaWlp5rLenzp1yu/+vKth3jYABod7/pxPt/772h6PCaKoqCgpLy/3naupqTFL63pOSNN7PWxraGjwtSkrKzPL9nroBmDw6FPPR8+9PProo2YSuaWlRV5//XU5evSovPvuu2YiOD8/XzZs2CDJyckmUNasWWMCZ86cOeb28+fPNyGzfPly2bZtm5nn2bRpk/lsEMMqYHDpU/joHstPf/pT+fzzz03Y6A8c6uD53ve+Z85v375dnE6n+XCh7g3plaxdu3b5bh8RESEHDx6UlStXmlAaMmSImTPasmVL/z8zAEHNodfbJcTopXkdfnrSW/ewAITee5Pf7QJgBeEDwArCB4AVhA8AKwgfAFYQPgCsIHwAWEH4ALCC8AFgBeEDwArCB4AVhA8AKwgfAFYQPgCsIHwAWEH4ALCC8AFgBeEDwArCB4AVhA8AKwgfAFYQPgCsIHwAWEH4ALCC8AFgBeEDwArCB4AVhA8AKwgfAFYQPgCsIHwAWEH4ALCC8AFgBeEDwArCB0Dohc/WrVvF4XDIunXrfNe1tbXJqlWrZPjw4ZKQkCCLFy+W+vp6v9tdvnxZcnNzJT4+XlJSUmTjxo3S2dl5L6UAGCzhc/r0afnd734nU6dO9bt+/fr1cuDAASkpKZFjx45JXV2dLFq0yHe+q6vLBE97e7ucOHFC9uzZI7t375bCwsJ7eyYAQou6Cy0tLWrChAmqrKxMzZ07V61du9Zc39jYqKKiolRJSYmv7YULF5R+mIqKCnN86NAh5XQ6ldvt9rUpLi5WLpdLeTyeXh+vra1NNTU1+bba2lpzn/oygOCh35N3+t68q56PHlbp3kt2drbf9ZWVldLR0eF3/cSJEyUjI0MqKirMsd5PmTJFUlNTfW1ycnKkublZqqure328oqIiSUxM9G3p6el3UzaAINLn8Nm7d6+cPXvWBMLt3G63REdHS1JSkt/1Omj0OW+bnsHjPe8915uCggJpamrybbW1tX0tG0CQiexLY/2mX7t2rZSVlUlsbKwESkxMjNkADNKejx5WNTQ0yPTp0yUyMtJselJ5x44d5rLuweiJ5MbGRr/b6dWutLQ0c1nvb1/98h572wAIf30Kn3nz5sm5c+ekqqrKt82cOVOWLVvmuxwVFSXl5eW+29TU1Jil9aysLHOs9/o+dIh56Z6Uy+WSzMzM/nxuAMJl2DV06FCZPHmy33VDhgwxn+nxXp+fny8bNmyQ5ORkEyhr1qwxgTNnzhxzfv78+SZkli9fLtu2bTPzPJs2bTKT2AytgMGjT+FzJ7Zv3y5Op9N8uNDj8ZiVrF27dvnOR0REyMGDB2XlypUmlHR45eXlyZYtW/q7FABBzKHX2yXE6GV5veSuV7507wpA6L03+d0uAFYQPgCsIHwAWEH4ALCC8AFgBeEDwArCB4AVhA8AKwgfAFYQPgCsIHwAWEH4ALCC8AFgBeEDwArCB4AVhA8AKwgfAFYQPgCsIHwAWEH4ALCC8AFgBeEDwArCB4AVhA8AKwgfAFYQPgCsIHwAWEH4ALCC8AFgBeEDwArCB4AVhA8AKwgfAFYQPgCsIHwAWEH4ALCC8AFgRaSEIKWU2Tc3N9suBUAP3vek9z0aduFz/fp1s09PT7ddCoBetLS0SGJiooRd+CQnJ5v95cuXv/YJBuP/GXRo1tbWisvlklBB3YHVHKJ16x6PDp7Ro0d/bduQDB+n8/+mqnTwhNI/TE+67lCsnboDyxWCdd9ph4AJZwBWED4ArAjJ8ImJiZHnnnvO7ENNqNZO3YEVE6J194VD3cmaGAD0s5Ds+QAIfYQPACsIHwBWED4ArCB8AFgRkuGzc+dOGTt2rMTGxsrs2bPl1KlTVus5fvy4PPbYY+Yj5Q6HQ/bv3+93Xi8oFhYWyqhRoyQuLk6ys7Plk08+8Wtz48YNWbZsmfk0a1JSkuTn58vNmzcHtO6ioiKZNWuWDB06VFJSUmThwoVSU1Pj16atrU1WrVolw4cPl4SEBFm8eLHU19f7tdG/5pKbmyvx8fHmfjZu3CidnZ0DVndxcbFMnTrV9+nfrKwsKS0tDeqae7N161bzelm3bl3I1d4vVIjZu3evio6OVn/4wx9UdXW1evLJJ1VSUpKqr6+3VtOhQ4fUr371K/WXv/xFf2xB7du3z+/81q1bVWJiotq/f7/629/+pn74wx+qcePGqVu3bvnaLFiwQE2bNk19+OGH6v3331fjx49XS5cuHdC6c3Jy1GuvvabOnz+vqqqq1Pe//32VkZGhbt686Wvz9NNPq/T0dFVeXq7OnDmj5syZo7797W/7znd2dqrJkyer7Oxs9dFHH5mfxYgRI1RBQcGA1f3Xv/5Vvf322+of//iHqqmpUb/85S9VVFSUeR7BWvPtTp06pcaOHaumTp2q1q5d67s+FGrvLyEXPg899JBatWqV77irq0uNHj1aFRUVqWBwe/h0d3ertLQ09cILL/iua2xsVDExMeqNN94wxx9//LG53enTp31tSktLlcPhUFeuXAlY7Q0NDaaOY8eO+erUb+qSkhJfmwsXLpg2FRUV5li/+J1Op3K73b42xcXFyuVyKY/HE7Dahw0bpl599dWQqLmlpUVNmDBBlZWVqblz5/rCJxRq708hNexqb2+XyspKM2zp+Uum+riiokKC0aVLl8TtdvvVrH/xTg8XvTXrvR5qzZw509dGt9fP7eTJkwGrtampye9bA/TPuqOjw6/2iRMnSkZGhl/tU6ZMkdTUVF+bnJwc81vZ1dXVA15zV1eX7N27V1pbW83wKxRq1sOq3Nxcvxq1UKi9P4XUb7Vfu3bNvNh6/uA1fXzx4kUJRjp4tN5q9p7Tez127ykyMtKEgLfNQOvu7jZzDw8//LBMnjzZV1d0dLQJxq+qvbfn5j03UM6dO2fCRs+R6LmRffv2SWZmplRVVQVtzZoOyrNnz8rp06f/41ww/7xlsIcPBvb/xufPn5cPPvhAQsEDDzxggkb31t58803Jy8uTY8eOSTDT382zdu1aKSsrM4slg11IDbtGjBghERER/zH7r4/T0tIkGHnr+qqa9b6hocHvvF690CtggXheq1evloMHD8qRI0dkzJgxfrXroW5jY+NX1t7bc/OeGyi6hzB+/HiZMWOGWbWbNm2avPjii0Fdsx5W6X/n6dOnm56t3nRg7tixw1zWPZhgrV0Ge/joF5x+sZWXl/sNF/Sx7oIHo3HjxpkXRc+a9fhcz+V4a9Z7/YLTL06vw4cPm+em54YGip4f18Gjhyz68XStPemfdVRUlF/teileL/X2rF0PgXqGp/4/u14C18OgQNE/K4/HE9Q1z5s3zzyu7rFV/XvT83z6Ixbey8Fa+4BQIbjUrleKdu/ebVaJnnrqKbPU3nP2P9D06oVe9tSb/pH+5je/MZc/++wz31K7rvGtt95Sf//739Xjjz/e61L7gw8+qE6ePKk++OADsxoy0EvtK1euNB8BOHr0qPr8889927/+9S+/pV+9/H748GGz9JuVlWW225d+58+fb5br33nnHTVy5MgBXfp99tlnzYrcpUuXzM9TH+uVwffeey9oa/4yc3usdoVa7fcq5MJHe+mll8w/kP68j15615+NsenIkSMmdG7f8vLyfMvtmzdvVqmpqSY4582bZz6f0tP169dN2CQkJJhl0xUrVphQG0i91aw3/dkfLx2QzzzzjFnKjo+PVz/60Y9MQPX06aefqkcffVTFxcWZz5z8/Oc/Vx0dHQNW9xNPPKHuu+8+8++v33j65+kNnmCt+U7D51YI1X6v+D4fAFaE1JwPgPBB+ACwgvABYAXhA8AKwgeAFYQPACsIHwBWED4ArCB8AFhB+ACwgvABIDb8L28J0OCHnou/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "146.6409149169922"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(True)[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop (3.9.23)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
