{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6bda73c",
   "metadata": {},
   "source": [
    "基本原理和离散动作是一样的,连续动作的概率使用高斯密度函数计算即可."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2df567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR8AAAEXCAYAAACUBEAgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbd0lEQVR4nO3dC1BU1/0H8N8ujxWEXQQEJICSamKoj9RHlNj5O/9IRWNMjKZjUscQwz8PRcdH6jS0SqaZzuBfp021GkybidrJqCm2mEg1CQHFOhIhGBJEpeYfE4i44CO7PIQFds9/fqfuDmvwscLu2WW/n5mby7333OUs7n5z7jn3oRFCCAIA8DCtp38hAABD+ACAEggfAFAC4QMASiB8AEAJhA8AKIHwAQAlED4AoATCBwCUQPgAgH+Fz7Zt22jEiBE0aNAgmjJlCpWXl6uqCgD4S/i89957tGbNGnrttdfo5MmTNH78eEpPT6empiYV1QEABTQqLizlls7kyZNp69atctlms1FiYiKtWLGCXn311dvuz+UbGhooPDycNBqNB2oMAHeC46SlpYXi4+NJq7112yaQPKyzs5MqKyspOzvbsY4rmZaWRmVlZb3uY7FY5GR34cIFSklJ8Uh9AcB19fX1lJCQ4F3hc/nyZbJarRQbG+u0npfPnj3b6z65ubn029/+ttc3qNfr3VZXAHBNc3OzPIrho5Lb8Xj43A1uJXEf0Y1vkIMH4QPgfe6kO8Tj4RMdHU0BAQHU2NjotJ6X4+Liet1Hp9PJCQAGDo+PdgUHB9PEiROpuLjYqQOZl1NTUz1dHQBQRMlhFx9CZWRk0KRJk+ihhx6iP/7xj9TW1kZLlixRUR0A8JfwWbhwIV26dIlycnLIaDTSgw8+SB9++OEPOqEBYOBScp5PX3GHs8FgILPZjA5nAB/9buLaLgBQAuEDAEogfABACYQPACiB8AEAJRA+AKAEwgcAlED4AIASCB8AUALhAwBKIHwAQAmEDwAogfABACUQPgCgBMIHAJRA+ACAEggfAFAC4QMASiB8AEAJhA8AKIHwAQAlED4AoATCBwCUQPgAgBIIHwBQAuEDAEogfABACYQPACiB8AEAJRA+AKAEwgcAlED4AIASCB8AUALhAwBKIHwAwDfC5+jRozR37lyKj48njUZD+/fvd9ouhKCcnBwaNmwYhYSEUFpaGp07d86pzNWrV2nRokWk1+spIiKCMjMzqbW1te/vBgAGbvi0tbXR+PHjadu2bb1u37hxI23ZsoW2b99OJ06coMGDB1N6ejp1dHQ4ynDw1NTUUFFRERUWFspAe/HFF/v2TgDAt4g+4N0LCgocyzabTcTFxYlNmzY51plMJqHT6cSePXvk8unTp+V+FRUVjjKHDh0SGo1GXLhw4Y5+r9lslq/BcwDwHq58N/u1z+f8+fNkNBrloZadwWCgKVOmUFlZmVzmOR9qTZo0yVGGy2u1WtlS6o3FYqHm5manCQB8W7+GDwcPi42NdVrPy/ZtPI+JiXHaHhgYSJGRkY4yN8rNzZUhZp8SExP7s9oAoIBPjHZlZ2eT2Wx2TPX19aqrBADeFD5xcXFy3tjY6LSel+3beN7U1OS0vbu7W46A2cvcSKfTyZGxnhMA+LZ+DZ/k5GQZIMXFxY513D/DfTmpqalymecmk4kqKysdZUpKSshms8m+IQDwD4Gu7sDn43z11VdOncxVVVWyzyYpKYlWrVpFv/vd72jUqFEyjNavXy/PCZo3b54s/8ADD9CsWbPohRdekMPxXV1dtHz5cnr66adlOQDwE64OpR0+fFgOpd04ZWRkOIbb169fL2JjY+UQ+4wZM0Rtba3Ta1y5ckU888wzIiwsTOj1erFkyRLR0tLiluE8APAcV76bGv4P+Rg+lONRL+58Rv8PgG9+N31itAsABh6EDwAogfABACUQPgCgBMIHAJRA+ACAEggfAFAC4QMASiB8AEAJhA8AKIHwAQAlED4AoATCBwCUQPgAgBIIHwBQAuEDAEogfABACYQPACiB8AEAJRA+AKAEwgcAlED4AIASCB8AUALhAwBKIHwAQAmEDwAogfABACUQPgCgBMIHAJRA+ACAEggfAFAC4QMASiB8AEAJhA8AKIHwAQDvD5/c3FyaPHkyhYeHU0xMDM2bN49qa2udynR0dFBWVhZFRUVRWFgYLViwgBobG53K1NXV0Zw5cyg0NFS+ztq1a6m7u7t/3hEADLzwKS0tlcHy6aefUlFREXV1ddHMmTOpra3NUWb16tV04MABys/Pl+UbGhpo/vz5ju1Wq1UGT2dnJx0/fpx27dpFO3fupJycnP59ZwDg3UQfNDU1CX6J0tJSuWwymURQUJDIz893lDlz5owsU1ZWJpcPHjwotFqtMBqNjjJ5eXlCr9cLi8VyR7/XbDbL1+Q5AHgPV76bferzMZvNch4ZGSnnlZWVsjWUlpbmKDN69GhKSkqisrIyuczzsWPHUmxsrKNMeno6NTc3U01NTa+/x2KxyO09JwDwbXcdPjabjVatWkXTpk2jMWPGyHVGo5GCg4MpIiLCqSwHDW+zl+kZPPbt9m0362syGAyOKTEx8W6rDQC+Hj7c93Pq1Cnau3cvuVt2drZsZdmn+vp6t/9OAHCvwLvZafny5VRYWEhHjx6lhIQEx/q4uDjZkWwymZxaPzzaxdvsZcrLy51ezz4aZi9zI51OJycA8NOWjxBCBk9BQQGVlJRQcnKy0/aJEydSUFAQFRcXO9bxUDwPraempsplnldXV1NTU5OjDI+c6fV6SklJ6fs7AoCB1/LhQ63du3fT+++/L8/1sffRcD9MSEiInGdmZtKaNWtkJzQHyooVK2TgTJ06VZbloXkOmcWLF9PGjRvla6xbt06+Nlo3AH7ElWE0Lt7btGPHDkeZ9vZ2sWzZMjFkyBARGhoqnnzySXHx4kWn1/nmm2/E7NmzRUhIiIiOjhavvPKK6OrqcstwHgB4jivfTQ3/h3wMD7VzK4s7n7l1BQDewZXvJq7tAgAlED4AoATCBwCUQPgAgBIIHwBQAuEDAEogfABACYQPACiB8AEA37mqHaA/CJuNrG1tZOvsJE1AAAWEhJAmOJg0Go3qqoEHIHzA4/iKnq6rV+nSoUNkLi+nzsuXSavTUejIkRTz6KMUPm6cDCMY2BA+4PHgsTQ00DebN1MbP/nk+qWF1tZWMl+5Qq3V1RT/7LM0ND0dATTAIXzAo8HTevo0fbt1K1kuXJDLps5O+r+WFjIEB9OPwsOJrl2jC3/9KwUPHUqGSZNwCDaAIXzAIzhoOo1GOv/731PX5ctyua6tjdZ//jnVms00ODCQ/ue++2gh36Du2jVqLCig8DFjZD8QDEwY7QKPsLW304V335XBw/hg63+rq+m0yURWIai5q4u2njlDp77/Xm6/9vXXZG1vV1xrcCeED7idsFplS+b7Y8ec1nPg9NRps5HFavVw7UAVhA+4FR9emSsrqamw0NG5zLgn57/j4iiwR5/OfXo9DQ8Lkz8H6HSk0eLjOZChzwfcO7JlNFL9n/8sz+fpiTuSM0aOpPCgIPrk4kUaFhJCL9x3H8UMGiS3D5k+nQJxl8oBDeEDbsOBU//WW9TZ40klPQVqtfTzESPoqREjZEvIHkqBERE0dNYstHwGOIQPuIXo7qbv3nmHmk+evGU5DhunwXStluJ/8QvS3eQZbjBw4H8t4JbLJq4eO/aDDubb0mgoOj2doh55BK0eP4B/Yej3fp5r589T/V/+QraODpf2Db33XopftIi0wcFuqx94D4QP9Kvu77+n795+m6wtLS7tFxwbS8NXrKBAPssZ/ALCB/qNrauLGvbsodaaGpf242u44p56ikKSk3E5hR9B+EC/HW5d+eQTulJS4tqOWi3Fzp9P0TNmIHj8DMIH+qef56uvqGH3bhI3nLV8O/oHH5ThownEwKu/QfhAn/H1WnV5edRtNru0n+6eeyghM5MCBw92W93AeyF8oE9sFgvVbd8uWz6u0AQFUcJzz9GghAS31Q28G8IH+nbB6Acf3PZEwt46mPlEQtyvx78hfOCu+3lavvySjH/7mwwhV/BtUmMeewx3KvRzCB+461uhyhMJLRaX9g390Y/k+Tx8o3jwbwgfcBmfucy3Ou347juX9gsIC6PEl16ioKgoHG4Bwgdcw4dYxr//nUwnTri0H7d07lm8mAbfdx+CBySED9zdjcFsNpf2jfzpTymKTyTEBaNwHT4JcMfav/5a3hjMdu2aS/uF/fjHdE9GBi4YBScIH7gj3a2tVHeLG4PdTEBoKN3z3HPyBmEAdx0+eXl5NG7cONLr9XJKTU2lQ4cOObZ3dHRQVlYWRUVFUVhYGC1YsIAaGxudXqOuro7mzJlDoaGhFBMTQ2vXrqXu7m5XqgEexo8zbnj33f885M8F/BTSpKws9PNA38MnISGBNmzYQJWVlfTZZ5/RI488Qk888QTVXL+KefXq1XTgwAHKz8+n0tJSamhooPnz5zv2t1qtMng6Ozvp+PHjtGvXLtq5cyfl5OS4Ug3wcD/P98eP0+WPP3a6AfydGDp3Lg15+GEED/RKI/jT1QeRkZG0adMmeuqpp2jo0KG0e/du+TM7e/YsPfDAA1RWVkZTp06VraTHHntMhlJsbKwss337dvrVr35Fly5douCb9AlYLBY52TU3N1NiYiKZzWbZAgP34I8Gt3a+zs2lruvP07pThsmTacTq1RR4/WkU4B+am5vJYDDc0Xfzrvt8uBWzd+9eamtrk4df3Brq6uqitLQ0R5nRo0dTUlKSDB/G87FjxzqCh6Wnp8sK21tPvcnNzZVvyD5x8ID78YWiF3btcjl4gqKj5QWjAbhgFPozfKqrq2V/jk6no5dffpkKCgooJSWFjEajbLlE3NCxyEHD2xjPewaPfbt9281kZ2fLJLVP9fX1rlYbXMRPC/1uxw75bHVXTyRMWrqUdMOG4XALbsnlm6jcf//9VFVVJUNg3759lJGRIft33ImDjifw3OEW9/FcPXLEtX6e6zeAN0yYgOCB/g8fbt2MHDlS/jxx4kSqqKigzZs308KFC2VHsslkcmr98GhX3PXHoPC8vLzc6fXso2H2MqA+ePgq9YvvvedyB3Pkf/0XDVu4EBeMgmfO87Hx87UtFhlEQUFBVFxc7NhWW1srh9a5T4jxnA/bmnqcK1JUVCQ7pvjQDdTrunJFPm/L2trq0n6Dhg+X5/MEXH/iKEC/tny472X27NmyE7mlpUWObB05coQ++ugj2RGcmZlJa9askSNgHCgrVqyQgcMjXWzmzJkyZBYvXkwbN26U/Tzr1q2T5wbhsEo967Vr8kTCDhf71AINBkp66SUKjopyW93Az8OHWyzPPvssXbx4UYYNn3DIwfOzn/1Mbn/jjTdIq9XKkwu5NcQjWW+++aZj/4CAACosLKSlS5fKUBo8eLDsM3r99df7/52ByxeMXvroIzJXVLi2o1YrD7X4EgoAj57n4+3nEsAdnkh47Bh9u22ba9dtaTQ0dPZsSnj+eVy3BZ47zwcG0I3BLl6UD/pz9YLRQUlJNOzpp+X9mAFchfDxc93NzfLJE66eSBg8dCgl8xnMBgOG1eGuIHz8/Qbw+/dTyxdfuLQfP2PrniVL8IRR6BOEjz8/YbSkhJoOHHBtR62WYh5/nCKmTEHwQJ8gfPxU+7ffyueqi85Ol/bjUa24n/+ctOjngT5C+Pgh7t+p27pVPmnUFfyAv+FLl+IJo9AvED5+xtbVRcZ9+6jt3DmX9uMRrfhFi+QjjgH6A8LHz/p52s+fp8uffOLSdVv2J4xGpKainwf6DcLHn1it8skTtvZ2l3aLmDaNhvITRvHkCehH+DT5EVt3N7Xe4qZtN3vC6D3PPksBuPYO+hnCB25KGxIi70jIJxQC9DeEj5/hJ0rcaQdzwnPPUVhKCvp5wC0QPn4WPPzUUL4g9Hb4JMKotDT084Db4JPlR7gFEz1zJuknTLh5oYAA+eSJxJdewomE4FYIHz/DN3gfvmyZHMG68Wp0ftpE7Lx5lPzLX1KQwaCsjuAfXL6HM/h+64c7kEesXCmfTNHy5ZfU3dIi1/GN30PvvVdeOArgbviU+Sm+1zKHDU8AKuCwCwCUQPgAgBIIHwBQAuEDAEogfABACYQPACiB8AEAJRA+AKAEwgcAlED4AIASCB8AUALhAwBKIHwAQAmEDwAogfABACUQPgCgBMIHAJRA+ACA74XPhg0b5D2BV61a5VjX0dFBWVlZFBUVRWFhYbRgwQJqbGx02q+uro7mzJlDoaGhFBMTQ2vXrqXu7u6+VAUA/CV8Kioq6K233qJx48Y5rV+9ejUdOHCA8vPzqbS0lBoaGmj+/PmO7VarVQZPZ2cnHT9+nHbt2kU7d+6knJycvr0TAPAt4i60tLSIUaNGiaKiIjF9+nSxcuVKud5kMomgoCCRn5/vKHvmzBnBv6asrEwuHzx4UGi1WmE0Gh1l8vLyhF6vFxaLpdff19HRIcxms2Oqr6+Xr8k/A4D34O/knX4376rlw4dV3HpJS0tzWl9ZWUldXV1O60ePHk1JSUlUVlYml3k+duxYio2NdZRJT0+n5uZmqqmp6fX35ebmksFgcEyJiYl3U20A8CIuh8/evXvp5MmTMhBuZDQaKTg4mCIiIpzWc9DwNnuZnsFj327f1pvs7Gwym82Oqb6+3tVqA4AvP7eLv/QrV66koqIiGjRoEHmKTqeTEwD4acuHD6uamppowoQJFBgYKCfuVN6yZYv8mVsw3JFsMpmc9uPRrri4OPkzz28c/bIv28sAwMDnUvjMmDGDqqurqaqqyjFNmjSJFi1a5Pg5KCiIiouLHfvU1tbKofXU1FS5zHN+DQ4xO25J6fV6SklJ6c/3BgAD5bArPDycxowZ47Ru8ODB8pwe+/rMzExas2YNRUZGykBZsWKFDJypU6fK7TNnzpQhs3jxYtq4caPs51m3bp3sxMahFYD/6Pdntb/xxhuk1WrlyYUWi0WOZL355puO7QEBAVRYWEhLly6VocThlZGRQa+//np/VwUAvJiGx9vJx/CwPA+588gXt64AwPe+m7i2CwCUQPgAgBIIHwBQAuEDAEogfABACYQPACiB8AEAJRA+AKAEwgcAlED4AIASCB8AUALhAwBKIHwAQAmEDwAogfABACUQPgCgBMIHAJRA+ACAEggfAFAC4QMASiB8AEAJhA8AKIHwAQAlED4AoATCBwCUQPgAgBIIHwBQAuEDAEogfABACYQPACiB8AEAJRA+AKAEwgcAlED4AIASCB8AUALhAwBKIHwAQIlA8kFCCDlvbm5WXRUA6MH+nbR/Rwdc+Fy5ckXOExMTVVcFAHrR0tJCBoOBBlz4REZGynldXd1t36A3/p+BQ7O+vp70ej35CtTbs5p9tN7c4uHgiY+Pv21ZnwwfrfY/XVUcPL70D9MT19sX6456e5beB+t9pw0CdDgDgBIIHwBQwifDR6fT0WuvvSbnvsZX6456e5bOR+vtCo24kzExAIB+5pMtHwDwfQgfAFAC4QMASiB8AEAJhA8AKOGT4bNt2zYaMWIEDRo0iKZMmULl5eVK63P06FGaO3euPKVco9HQ/v37nbbzgGJOTg4NGzaMQkJCKC0tjc6dO+dU5urVq7Ro0SJ5NmtERARlZmZSa2urW+udm5tLkydPpvDwcIqJiaF58+ZRbW2tU5mOjg7KysqiqKgoCgsLowULFlBjY6NTGb7MZc6cORQaGipfZ+3atdTd3e22eufl5dG4ceMcZ/+mpqbSoUOHvLrOvdmwYYP8vKxatcrn6t4vhI/Zu3evCA4OFu+8846oqakRL7zwgoiIiBCNjY3K6nTw4EHxm9/8RvzjH//g0xZEQUGB0/YNGzYIg8Eg9u/fL7744gvx+OOPi+TkZNHe3u4oM2vWLDF+/Hjx6aefin/9619i5MiR4plnnnFrvdPT08WOHTvEqVOnRFVVlXj00UdFUlKSaG1tdZR5+eWXRWJioiguLhafffaZmDp1qnj44Ycd27u7u8WYMWNEWlqa+Pzzz+XfIjo6WmRnZ7ut3h988IH45z//Kf7973+L2tpa8etf/1oEBQXJ9+Gtdb5ReXm5GDFihBg3bpxYuXKlY70v1L2/+Fz4PPTQQyIrK8uxbLVaRXx8vMjNzRXe4MbwsdlsIi4uTmzatMmxzmQyCZ1OJ/bs2SOXT58+LferqKhwlDl06JDQaDTiwoULHqt7U1OTrEdpaamjnvylzs/Pd5Q5c+aMLFNWViaX+cOv1WqF0Wh0lMnLyxN6vV5YLBaP1X3IkCHi7bff9ok6t7S0iFGjRomioiIxffp0R/j4Qt37k08ddnV2dlJlZaU8bOl5kSkvl5WVkTc6f/48GY1GpzrzhXd8uGivM8/5UGvSpEmOMlye39uJEyc8Vlez2ex01wD+W3d1dTnVffTo0ZSUlORU97Fjx1JsbKyjTHp6urwqu6amxu11tlqttHfvXmpra5OHX75QZz6smjNnjlMdmS/UvT/51FXtly9flh+2nn94xstnz54lb8TBw3qrs30bz/nYvafAwEAZAvYy7maz2WTfw7Rp02jMmDGOegUHB8tgvFXde3tv9m3uUl1dLcOG+0i4b6SgoIBSUlKoqqrKa+vMOChPnjxJFRUVP9jmzX9v8vfwAff+3/jUqVN07Ngx8gX333+/DBpure3bt48yMjKotLSUvBnfm2flypVUVFQkB0v8nU8ddkVHR1NAQMAPev95OS4ujryRvV63qjPPm5qanLbz6AWPgHnifS1fvpwKCwvp8OHDlJCQ4FR3PtQ1mUy3rHtv782+zV24hTBy5EiaOHGiHLUbP348bd682avrzIdV/O88YcIE2bLliQNzy5Yt8mduwXhr3cnfw4c/cPxhKy4udjpc4GVugnuj5ORk+aHoWWc+Pue+HHudec4fOP5w2pWUlMj3xn1D7sL94xw8fMjCv4/r2hP/rYOCgpzqzkPxPNTbs+58CNQzPPn/7DwEzodBnsJ/K4vF4tV1njFjhvy93GKruj5xPx+fYmH/2Vvr7hbCB4faeaRo586dcpToxRdflEPtPXv/PY1HL3jYkyf+k/7hD3+QP3/77beOoXau4/vvvy++/PJL8cQTT/Q61P6Tn/xEnDhxQhw7dkyOhrh7qH3p0qXyFIAjR46IixcvOqZr1645Df3y8HtJSYkc+k1NTZXTjUO/M2fOlMP1H374oRg6dKhbh35fffVVOSJ3/vx5+ffkZR4Z/Pjjj722zjczvcdol6/Vva98LnzYn/70J/kPxOf78NA7nxuj0uHDh2Xo3DhlZGQ4htvXr18vYmNjZXDOmDFDnp/S05UrV2TYhIWFyWHTJUuWyFBzp97qzBOf+2PHAbls2TI5lB0aGiqefPJJGVA9ffPNN2L27NkiJCREnnPyyiuviK6uLrfV+/nnnxfDhw+X//78xeO/pz14vLXOdxo+7T5U977C/XwAQAmf6vMBgIED4QMASiB8AEAJhA8AKIHwAQAlED4AoATCBwCUQPgAgBIIHwBQAuEDAEogfACAVPh/saUyyCQvZ9gAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "#定义环境\n",
    "class MyWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self):\n",
    "        env = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.step_n = 0\n",
    "\n",
    "    def reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        self.step_n = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, terminated, truncated, info = self.env.step(\n",
    "            [action * 2])\n",
    "        over = terminated or truncated\n",
    "\n",
    "        #偏移reward,便于训练\n",
    "        reward = (reward + 8) / 8\n",
    "\n",
    "        #限制最大步数\n",
    "        self.step_n += 1\n",
    "        if self.step_n >= 200:\n",
    "            over = True\n",
    "\n",
    "        return state, reward, over\n",
    "\n",
    "    #打印游戏图像\n",
    "    def show(self):\n",
    "        from matplotlib import pyplot as plt\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(self.env.render())\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()\n",
    "\n",
    "env.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "967d2307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[0.0291],\n",
       "          [0.0673]], grad_fn=<TanhBackward0>),\n",
       "  tensor([[0.8901],\n",
       "          [0.8493]], grad_fn=<ExpBackward0>)),\n",
       " tensor([[ 0.0453],\n",
       "         [-0.1344]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "#定义模型\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.s = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.mu = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64, 1),\n",
    "            torch.nn.Tanh(),\n",
    "        )\n",
    "        self.sigma = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64, 1),\n",
    "            torch.nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = self.s(state)\n",
    "\n",
    "        return self.mu(state), self.sigma(state).exp()\n",
    "\n",
    "\n",
    "model_action = Model()\n",
    "\n",
    "model_value = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 1),\n",
    ")\n",
    "\n",
    "model_action(torch.randn(2, 3)), model_value(torch.randn(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d5fdc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\it_project\\github_sync\\ml-workshop\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_44956\\3994207745.py:34: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  state = torch.FloatTensor(state).reshape(-1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30.417152404785156"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "import random\n",
    "\n",
    "\n",
    "#玩一局游戏并记录数据\n",
    "def play(show=False):\n",
    "    state = []\n",
    "    action = []\n",
    "    reward = []\n",
    "    next_state = []\n",
    "    over = []\n",
    "\n",
    "    s = env.reset()\n",
    "    o = False\n",
    "    while not o:\n",
    "        #根据概率采样\n",
    "        mu, sigma = model_action(torch.FloatTensor(s).reshape(1, 3))\n",
    "        a = random.normalvariate(mu=mu.item(), sigma=sigma.item())\n",
    "\n",
    "        ns, r, o = env.step(a)\n",
    "\n",
    "        state.append(s)\n",
    "        action.append(a)\n",
    "        reward.append(r)\n",
    "        next_state.append(ns)\n",
    "        over.append(o)\n",
    "\n",
    "        s = ns\n",
    "\n",
    "        if show:\n",
    "            display.clear_output(wait=True)\n",
    "            env.show()\n",
    "\n",
    "    state = torch.FloatTensor(state).reshape(-1, 3)\n",
    "    action = torch.FloatTensor(action).reshape(-1, 1)\n",
    "    reward = torch.FloatTensor(reward).reshape(-1, 1)\n",
    "    next_state = torch.FloatTensor(next_state).reshape(-1, 3)\n",
    "    over = torch.LongTensor(over).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over, reward.sum().item()\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over, reward_sum = play()\n",
    "\n",
    "reward_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a22c5ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_action = torch.optim.Adam(model_action.parameters(), lr=5e-4)\n",
    "optimizer_value = torch.optim.Adam(model_value.parameters(), lr=5e-3)\n",
    "\n",
    "\n",
    "def requires_grad(model, value):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9657c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_value(state, reward, next_state, over):\n",
    "    requires_grad(model_action, False)\n",
    "    requires_grad(model_value, True)\n",
    "\n",
    "    #计算target\n",
    "    with torch.no_grad():\n",
    "        target = model_value(next_state)\n",
    "    target = target * 0.98 * (1 - over) + reward\n",
    "\n",
    "    #每批数据反复训练10次\n",
    "    for _ in range(10):\n",
    "        #计算value\n",
    "        value = model_value(state)\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(value, target)\n",
    "        loss.backward()\n",
    "        optimizer_value.step()\n",
    "        optimizer_value.zero_grad()\n",
    "\n",
    "    #减去value相当于去基线\n",
    "    return (target - value).detach()\n",
    "\n",
    "\n",
    "value = train_value(state, reward, next_state, over)\n",
    "\n",
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b92ff9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01315974723547697"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_action(state, action, value):\n",
    "    requires_grad(model_action, True)\n",
    "    requires_grad(model_value, False)\n",
    "\n",
    "    #计算优势函数\n",
    "    delta = []\n",
    "    for i in range(len(value)):\n",
    "        s = 0\n",
    "        for j in range(i, len(value)):\n",
    "            s += value[j] * (0.9 * 0.9)**(j - i)\n",
    "        delta.append(s)\n",
    "    delta = torch.FloatTensor(delta).reshape(-1, 1)\n",
    "\n",
    "    #更新前的动作概率\n",
    "    with torch.no_grad():\n",
    "        mu, sigma = model_action(state)\n",
    "        prob_old = torch.distributions.Normal(mu, sigma).log_prob(action).exp()\n",
    "\n",
    "    #每批数据反复训练10次\n",
    "    for _ in range(10):\n",
    "        #更新后的动作概率\n",
    "        mu, sigma = model_action(state)\n",
    "        prob_new = torch.distributions.Normal(mu, sigma).log_prob(action).exp()\n",
    "\n",
    "        #求出概率的变化\n",
    "        ratio = prob_new / prob_old\n",
    "\n",
    "        #计算截断的和不截断的两份loss,取其中小的\n",
    "        surr1 = ratio * delta\n",
    "        surr2 = ratio.clamp(0.8, 1.2) * delta\n",
    "\n",
    "        loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        #更新参数\n",
    "        loss.backward()\n",
    "        optimizer_action.step()\n",
    "        optimizer_action.zero_grad()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "train_action(state, action, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b2ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model_action.train()\n",
    "    model_value.train()\n",
    "\n",
    "    #训练N局\n",
    "    for epoch in range(1000):\n",
    "        #一个epoch最少玩N步\n",
    "        steps = 0\n",
    "        while steps < 200:\n",
    "            state, action, reward, next_state, over, _ = play()\n",
    "            steps += len(state)\n",
    "\n",
    "            #训练两个模型\n",
    "            delta = train_value(state, reward, next_state, over)\n",
    "            loss = train_action(state, action, delta)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            test_result = sum([play()[-1] for _ in range(20)]) / 20\n",
    "            print(epoch, loss, test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop (3.9.23)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
