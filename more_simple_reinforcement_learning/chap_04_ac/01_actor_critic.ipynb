{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20f173e",
   "metadata": {},
   "source": [
    "Reinforce算法中是以蒙特卡洛采样法来估计Q(state,action)的\n",
    "\n",
    "Actor_Critic算法中以神经网络来估算Q(state,action)\n",
    "\n",
    "再使用td误差来训练该神经网络\n",
    "\n",
    "\n",
    "'''\n",
    "。它點出了 Actor-Critic (AC) 演算法與前一章 REINFORCE 演算法的關鍵區別：\n",
    "\n",
    "   1. REINFORCE 的做法：它需要跑完一整局遊戲 (Monte Carlo)，然後用整局的未來總獎勵 $G_t$ 來當作對 $Q(s_t, a_t)$\n",
    "      的估計。這種方法雖然無偏差，但變異數很大，訓練不穩定。\n",
    "\n",
    "   2. Actor-Critic 的做法：它引入了一個獨立的評審員 (Critic) 網路，這個網路的任務就是直接學習去估算 $Q(s, a)$ 或狀態價值\n",
    "      $V(s)$。這樣就不需要等到整局遊戲結束。\n",
    "\n",
    "   3. 訓練方式：Critic 網路本身是透過時間差分 (TD) 誤差來訓練的，這和 Q-Learning 或 SARSA 的更新方式很類似，使得 AC\n",
    "      演算法可以進行單步更新，學習更穩定、更高效。\n",
    "\n",
    "  總結來說，這個儲存格點明了 AC 演算法的核心精神：用一個 Critic 網路來取代 REINFORCE\n",
    "  中的蒙地卡羅採樣，以降低變異數並提高學習效率。好的，我們接著看下一個儲存格。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abdd52c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "#定义环境\n",
    "class MyWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self):\n",
    "        env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.step_n = 0\n",
    "\n",
    "    def reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        self.step_n = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        over = terminated or truncated\n",
    "\n",
    "        #限制最大步数\n",
    "        self.step_n += 1\n",
    "        if self.step_n >= 200:\n",
    "            over = True\n",
    "        \n",
    "        #没坚持到最后,扣分\n",
    "        if over and self.step_n < 200:\n",
    "            reward = -1000\n",
    "\n",
    "        return state, reward, over\n",
    "\n",
    "    #打印游戏图像\n",
    "    def show(self):\n",
    "        from matplotlib import pyplot as plt\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(self.env.render())\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()\n",
    "\n",
    "env.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8557bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Sequential(\n",
       "   (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "   (3): ReLU()\n",
       "   (4): Linear(in_features=64, out_features=2, bias=True)\n",
       "   (5): Softmax(dim=1)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "   (3): ReLU()\n",
       "   (4): Linear(in_features=64, out_features=1, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#演员模型,计算每个动作的概率\n",
    "model_actor = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 2),\n",
    "    torch.nn.Softmax(dim=1),\n",
    ")\n",
    "\n",
    "#评委模型,计算每个状态的价值\n",
    "model_critic = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 1),\n",
    ")\n",
    "\n",
    "model_critic_delay = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 1),\n",
    ")\n",
    "\n",
    "model_critic_delay.load_state_dict(model_critic.state_dict())\n",
    "\n",
    "model_actor, model_critic\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "  這個儲存格定義了 Actor-Critic (AC) 架構所需的三個核心神經網路模型。\n",
    "\n",
    "   1. `model_actor` (演員模型):\n",
    "       * 功能：這是策略 (Policy) 網路。它的任務是接收一個狀態 (state)，並輸出一組描述每個可能動作的機率。\n",
    "       * 輸入：CartPole 環境的狀態，是一個包含 4 個數字的向量 (車位置、車速度、桿角度、桿角速度)。\n",
    "       * 輸出：一個包含 2 個數字的向量，分別代表向左和向右移動的機率。\n",
    "       * 關鍵層：最後一層是 Softmax，它能確保輸出值是介於 0 和 1 之間的機率，且總和為 1。\n",
    "\n",
    "   2. `model_critic` (評審模型):\n",
    "       * 功能：這是價值 (Value) 網路。它的任務是接收一個狀態，並評估這個狀態有多好，也就是估算這個狀態的價值 (State Value, V(s))。\n",
    "       * 輸入：同樣是 4 維的狀態向量。\n",
    "       * 輸出：一個純量 (scalar)，代表對當前狀態的價值評分。\n",
    "       * 注意：在這個版本的 AC 中，Critic 估計的是 $V(s)$，而不是 $Q(s, a)$。\n",
    "\n",
    "   3. `model_critic_delay` (延遲評審模型):\n",
    "       * 功能：這是 Critic 模型的目標網路 (Target Network)，結構與 model_critic 完全相同。\n",
    "       * 目的：它的作用與 DQN 中的 Target Network 一樣。在計算 TD 目標值時，我們使用這個參數更新較慢的 delay 模型來提供一個穩定的目標，避免因\n",
    "         model_critic 自身參數的快速變動而導致的訓練震盪。\n",
    "       * 初始化：model_critic_delay.load_state_dict(model_critic.state_dict()) 這行程式碼確保了在訓練開始時，目標網路的權重與主網路完全一致。\n",
    "\n",
    "  總結來說，這個儲存格搭建了 AC 演算法的大腦：一個負責做決策的 Actor，以及一對負責評估決策好壞的 Critic 網路 (主網路和目標網路)。\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6aa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\it_project\\github_sync\\ml-workshop\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_54748\\2154798901.py:34: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  state = torch.FloatTensor(state).reshape(-1, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-971.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "import random\n",
    "\n",
    "\n",
    "#玩一局游戏并记录数据\n",
    "def play(show=False):\n",
    "    state = []\n",
    "    action = []\n",
    "    reward = []\n",
    "    next_state = []\n",
    "    over = []\n",
    "\n",
    "    s = env.reset()\n",
    "    o = False\n",
    "    while not o:\n",
    "        #根据概率采样\n",
    "        prob = model_actor(torch.FloatTensor(s).reshape(1, 4))[0].tolist()\n",
    "        a = random.choices(range(2), weights=prob, k=1)[0]\n",
    "\n",
    "        ns, r, o = env.step(a)\n",
    "\n",
    "        state.append(s)\n",
    "        action.append(a)\n",
    "        reward.append(r)\n",
    "        next_state.append(ns)\n",
    "        over.append(o)\n",
    "\n",
    "        s = ns\n",
    "\n",
    "        if show:\n",
    "            display.clear_output(wait=True)\n",
    "            env.show()\n",
    "\n",
    "    state = torch.FloatTensor(state).reshape(-1, 4)\n",
    "    action = torch.LongTensor(action).reshape(-1, 1)\n",
    "    reward = torch.FloatTensor(reward).reshape(-1, 1)\n",
    "    next_state = torch.FloatTensor(next_state).reshape(-1, 4)\n",
    "    over = torch.LongTensor(over).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over, reward.sum().item()\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over, reward_sum = play()\n",
    "\n",
    "reward_sum\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "  這個儲存格定義了 play() 函式，這是智能體 (Agent) 與環境 (Environment) 互動的核心。\n",
    "\n",
    "  函式功能：\n",
    "\n",
    "   1. 執行一場完整的遊戲：\n",
    "       * 從 env.reset() 開始，進入一個 while 迴圈，直到遊戲結束 (o 變為 True)。\n",
    "\n",
    "   2. 決策與行動：\n",
    "       * 在迴圈的每一步，它會：\n",
    "           * 將當前狀態 s 輸入到 model_actor 中，獲得一個動作機率分佈 prob。\n",
    "           * 使用 random.choices 根據這個機率分佈隨機採樣一個動作 a。這體現了策略梯度方法的核心——探索性。\n",
    "           * 執行動作 a，並從環境中獲得新的狀態 ns、獎勵 r 和結束標誌 o。\n",
    "\n",
    "   3. 記錄軌跡 (Trajectory)：\n",
    "       * 它會將每一步的 (s, a, r, ns, o) 這個五元組分別儲存在對應的 list 中。\n",
    "\n",
    "   4. 數據格式化：\n",
    "       * 遊戲結束後，它會將記錄下來的 Python list 轉換為 PyTorch 的 Tensor，並調整好形狀 (shape)，以便後續輸入到神經網路中進行訓練。\n",
    "\n",
    "   5. 回傳：\n",
    "       * 函式最終回傳遊戲過程中的所有數據（狀態、動作、獎勵等），以及這一整局遊戲的總獎勵 `reward_sum`。\n",
    "\n",
    "  執行部分：\n",
    "\n",
    "   * state, action, reward, next_state, over, reward_sum = play() 這一行執行了 play 函式，跑了一局遊戲。\n",
    "   * reward_sum 則顯示了這局遊戲的總得分。因為模型未經訓練，所以第一次玩的結果通常很差，得分會是一個很大的負數 (例如 -971.0)。\n",
    "\n",
    "  總結來說，這個儲存格的功能是數據採集。它定義了一個完整的互動循環，讓 Agent\n",
    "  根據當前策略玩遊戲，並把整個過程記錄下來，為接下來的訓練步驟準備好所需的數據。\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd655207",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_actor = torch.optim.Adam(model_actor.parameters(), lr=4e-3)\n",
    "optimizer_critic = torch.optim.Adam(model_critic.parameters(), lr=4e-2)\n",
    "\n",
    "\n",
    "def requires_grad(model, value):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f74555d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_critic(state, reward, next_state, over):\n",
    "    requires_grad(model_actor, False)\n",
    "    requires_grad(model_critic, True)\n",
    "\n",
    "    #计算values和targets\n",
    "    value = model_critic(state)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        target = model_critic_delay(next_state)\n",
    "    target = target * 0.99 * (1 - over) + reward\n",
    "\n",
    "    #时序差分误差,也就是tdloss\n",
    "    loss = torch.nn.functional.mse_loss(value, target)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_critic.step()\n",
    "    optimizer_critic.zero_grad()\n",
    "\n",
    "    return value.detach()\n",
    "\n",
    "\n",
    "value = train_critic(state, reward, next_state, over)\n",
    "\n",
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a219a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12820570170879364"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_actor(state, action, value):\n",
    "    requires_grad(model_actor, True)\n",
    "    requires_grad(model_critic, False)\n",
    "\n",
    "    #重新计算动作的概率\n",
    "    prob = model_actor(state)\n",
    "    prob = prob.gather(dim=1, index=action)\n",
    "\n",
    "    #根据策略梯度算法的导函数实现\n",
    "    #函数中的Q(state,action),这里使用critic模型估算\n",
    "    prob = (prob + 1e-8).log() * value\n",
    "    loss = -prob.mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_actor.step()\n",
    "    optimizer_actor.zero_grad()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "train_actor(state, action, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1248e2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -17.94116973876953 -979.4\n",
      "100 -298.44775390625 -936.5\n",
      "200 -240.7567901611328 -551.9\n",
      "300 -159.66920471191406 -169.65\n",
      "400 -124.30767059326172 39.25\n",
      "500 -49.67599105834961 149.1\n",
      "600 -56.38823699951172 200.0\n",
      "700 -29.849672317504883 96.75\n",
      "800 -22.606861114501953 200.0\n",
      "900 -15.701851844787598 149.6\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model_actor.train()\n",
    "    model_critic.train()\n",
    "\n",
    "    #训练N局\n",
    "    for epoch in range(1000):\n",
    "\n",
    "        #一个epoch最少玩N步\n",
    "        steps = 0\n",
    "        while steps < 200:\n",
    "            state, action, reward, next_state, over, _ = play()\n",
    "            steps += len(state)\n",
    "\n",
    "            #训练两个模型\n",
    "            value = train_critic(state, reward, next_state, over)\n",
    "            loss = train_actor(state, action, value)\n",
    "\n",
    "        #复制参数\n",
    "        for param, param_delay in zip(model_critic.parameters(),\n",
    "                                      model_critic_delay.parameters()):\n",
    "            value = param_delay.data * 0.7 + param.data * 0.3\n",
    "            param_delay.data.copy_(value)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            test_result = sum([play()[-1] for _ in range(20)]) / 20\n",
    "            print(epoch, loss, test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4168abc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-815.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(True)[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop (3.9.23)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
