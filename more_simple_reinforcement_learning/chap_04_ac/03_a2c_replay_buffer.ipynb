{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ae49ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class MyWrapper(gym.Wrapper):\n",
    "    def __init__(self):\n",
    "        env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.step_n = 0\n",
    "\n",
    "    def reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        self.step_n = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        over = terminated or truncated\n",
    "\n",
    "        self.step_n += 1\n",
    "        if self.step_n >= 200:\n",
    "            over = True\n",
    "        \n",
    "        if over and self.step_n < 200:\n",
    "            reward = -1000\n",
    "\n",
    "        return state, reward, over\n",
    "\n",
    "    def show(self):\n",
    "        from matplotlib import pyplot as plt\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(self.env.render())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1ac3e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = MyWrapper()\n",
    "\n",
    "env.reset()\n",
    "\n",
    "env.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7deb7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7514dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 演员模型,计算每个动作的概率\n",
    "model_actor = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 2),\n",
    "    torch.nn.Softmax(dim=1),\n",
    ")\n",
    "\n",
    "# 评委模型,计算每个状态的价值\n",
    "model_critic = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 1),\n",
    ")\n",
    "\n",
    "# 延遲評委模型, 用於穩定訓練\n",
    "model_critic_delay = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 1),\n",
    ")\n",
    "\n",
    "model_critic_delay.load_state_dict(model_critic.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c8c40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_actor = torch.optim.Adam(model_actor.parameters(), lr=1e-3)\n",
    "optimizer_critic = torch.optim.Adam(model_critic.parameters(), lr=1e-2)\n",
    "\n",
    "def requires_grad(model, value):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba8ebdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_critic(state, reward, next_state, over):\n",
    "    requires_grad(model_actor, False)\n",
    "    requires_grad(model_critic, True)\n",
    "\n",
    "    value = model_critic(state)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        target = model_critic_delay(next_state)\n",
    "    target = target * 0.99 * (1 - over) + reward\n",
    "\n",
    "    loss = torch.nn.functional.mse_loss(value, target)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_critic.step()\n",
    "    optimizer_critic.zero_grad()\n",
    "\n",
    "    return (target - value).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "817dc2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor(state, action, value):\n",
    "    requires_grad(model_actor, True)\n",
    "    requires_grad(model_critic, False)\n",
    "\n",
    "    prob = model_actor(state)\n",
    "    prob = prob.gather(dim=1, index=action)\n",
    "\n",
    "    prob = (prob + 1e-8).log() * value\n",
    "    loss = -prob.mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_actor.step()\n",
    "    optimizer_actor.zero_grad()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa3e1b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Total Reward: -981.0\n",
      "Epoch: 100, Total Reward: -884.0\n",
      "Epoch: 200, Total Reward: 200.0\n",
      "Epoch: 300, Total Reward: 200.0\n",
      "Epoch: 400, Total Reward: 200.0\n",
      "Epoch: 500, Total Reward: 200.0\n",
      "Epoch: 600, Total Reward: 200.0\n",
      "Epoch: 700, Total Reward: 200.0\n",
      "Epoch: 800, Total Reward: 200.0\n",
      "Epoch: 900, Total Reward: 200.0\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(capacity=10000)\n",
    "\n",
    "batch_size = 64\n",
    "soft_update_rate = 0.3\n",
    "\n",
    "for epoch in range(1000):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        with torch.no_grad():\n",
    "            prob = model_actor(torch.FloatTensor(state).reshape(1, 4))[0].tolist()\n",
    "        action = random.choices(range(2), weights=prob, k=1)[0]\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            \n",
    "            b_state, b_action, b_reward, b_next_state, b_done = replay_buffer.sample(batch_size)\n",
    "            \n",
    "            b_state = torch.FloatTensor(b_state).reshape(-1, 4)\n",
    "            b_action = torch.LongTensor(b_action).reshape(-1, 1)\n",
    "            b_reward = torch.FloatTensor(b_reward).reshape(-1, 1)\n",
    "            b_next_state = torch.FloatTensor(b_next_state).reshape(-1, 4)\n",
    "            b_done = torch.LongTensor(b_done).reshape(-1, 1)\n",
    "\n",
    "            advantage = train_critic(b_state, b_reward, b_next_state, b_done)\n",
    "            loss = train_actor(b_state, b_action, advantage)\n",
    "\n",
    "            for param, param_delay in zip(model_critic.parameters(), model_critic_delay.parameters()):\n",
    "                value = param_delay.data * (1.0 - soft_update_rate) + param.data * soft_update_rate\n",
    "                param_delay.data.copy_(value)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}, Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9b86a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from IPython import display\n",
    "    \n",
    "    def test_agent_and_show():\n",
    "        \"\"\"\n",
    "        此函式用於在訓練完成後, 觀看 agent 的表現。\n",
    "        \"\"\"\n",
    "        # 將模型設為評估模式\n",
    "        model_actor.eval()\n",
    "    \n",
    "        # 建立一個新的環境來顯示\n",
    "        env_show = MyWrapper()\n",
    "        state = env_show.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        print(\"--- 觀看訓練成果 ---\")\n",
    "        while not done:\n",
    "            # 顯示畫面\n",
    "            display.clear_output(wait=True)\n",
    "            env_show.show()\n",
    "    \n",
    "            # 選擇動作 (測試時通常選機率最高的動作，而不是隨機採樣)\n",
    "            with torch.no_grad():\n",
    "                prob = model_actor(torch.FloatTensor(state).reshape(1, 4))[0].tolist()\n",
    "                action = prob.index(max(prob))\n",
    "    \n",
    "            # 與環境互動\n",
    "            next_state, reward, done = env_show.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        print(f\"測試完成，總分: {total_reward}\")\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55c2f13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "測試完成，總分: 200.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_agent_and_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72784fc0",
   "metadata": {},
   "source": [
    " 在我們重構後的訓練程式碼中，已經不再需要 play() 函式了。\n",
    "\n",
    "  原因是：\n",
    "  play() 的作用是「一次性跑完一整局遊戲，並收集所有資料」。\n",
    "\n",
    "  但在新的架構中，我們的主訓練迴圈是一步一步地與環境互動，並將每一步的經驗 (state, action, reward...) 即時存入 Replay Buffer。然後，訓練是從\n",
    "  Replay Buffer 中抽樣，而不是使用 play() 收集的整局資料。\n",
    "\n",
    "  所以，新的訓練迴圈本身就包含了與環境互動的邏輯，完全取代了 play() 函式在訓練過程中的角色。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c48acc",
   "metadata": {},
   "source": [
    " 您不想要我之前寫的那種「邊玩邊學」的線上模式，而是想要一個更明確的兩階段過程：\n",
    "   1. 預填充階段：先進行大量的探索，用初始數據填滿數據池。\n",
    "   2. 訓練階段：從數據池中抽樣進行訓練，同時，每訓練一次，就去玩一局新的遊戲，把新的數據再放回池中，實現數據的持續更新。\n",
    "\n",
    "  這是一個很常見且有效的作法。您提到 chap01 有類似做法，為了完全符合您的思想，我將先讀取 chap01 的檔案來確認這個模式。\n",
    " ╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
    " │ ✔ ReadManyFiles Will attempt to read and concatenate files using patterns:…                                                       │\n",
    " │                                                                                                                                    │\n",
    " │                                                                                                                                    │\n",
    " │    ### ReadManyFiles Result (Target Dir: `D:\\it_project\\github_sync\\ml-workshop`)                                                  │\n",
    " │                                                                                                                                    │\n",
    " │    Successfully read and concatenated content from **3 file(s)**.                                                                  │\n",
    " │                                                                                                                                    │\n",
    " │    **Processed Files:**                                                                                                            │\n",
    " │    - `more_simple_reinforcement_learning/chap_01_q_learning_pools_and_sarsa/01_QLearning.ipynb`                                    │\n",
    " │    - `more_simple_reinforcement_learning/chap_01_q_learning_pools_and_sarsa/02_SARSA_no_pool.ipynb`                                │\n",
    " │    - `more_simple_reinforcement_learning/chap_01_q_learning_pools_and_sarsa/03_n_step_TD_learning.ipynb`                           │\n",
    " ╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
    "✦ 好的，我已經詳細閱讀了 chap01 的程式碼，特別是\n",
    "  01_QLearning.ipynb，現在完全理解您想要的「先用一批新數據更新數據池，然後再從池中抽樣訓練」的模式了。\n",
    "\n",
    "  這和我們之前討論的「即時線上學習」模式確實不同。我將為您重寫訓練迴圈，以符合這個新邏輯。\n",
    "\n",
    "  這個新模式需要一個輔助函式來「玩一局遊戲並收集數據」，我們將重新引入一個類似 play 的函式，但只用於數據收集。\n",
    "\n",
    "  ---\n",
    "\n",
    "  1. 輔助函式：`generate_episode_data`\n",
    "  這個函式負責玩一局遊戲，並返回該局的所有步驟數據。\n",
    "\n",
    "    1 def generate_episode_data():\n",
    "    2     \"\"\"\n",
    "    3     玩一局完整的遊戲, 並返回所有(s, a, r, ns, done)的list\n",
    "    4     \"\"\"\n",
    "    5     data = []\n",
    "    6     state = env.reset()\n",
    "    7     done = False\n",
    "    8     while not done:\n",
    "    9         with torch.no_grad():\n",
    "   10             prob = model_actor(torch.FloatTensor(state).reshape(1, 4))[0].tolist()\n",
    "   11         action = random.choices(range(2), weights=prob, k=1)[0]\n",
    "   12\n",
    "   13         next_state, reward, done = env.step(action)\n",
    "   14         data.append((state, action, reward, next_state, done))\n",
    "   15         state = next_state\n",
    "   16\n",
    "   17     return data\n",
    "\n",
    "  ---\n",
    "\n",
    "  2. 核心訓練迴圈 (新)\n",
    "  這就是您想要的，包含「預填充」和「訓練並更新」兩個階段的訓練迴圈。\n",
    "\n",
    "    1 def train_main():\n",
    "    2     # --- 超參數 ---\n",
    "    3     REPLAY_BUFFER_CAPACITY = 50000 # 數據池總容量\n",
    "    4     PREFILL_EPISODES = 200        # 預填充的遊戲局數 (您提到的10000局會非常久, 先用200局示範)\n",
    "    5\n",
    "    6     EPOCHS = 2000                 # 總訓練輪次\n",
    "    7     EPISODES_PER_EPOCH = 5        # 每一輪更新時, 要玩幾局新遊戲\n",
    "    8     TRAIN_STEPS_PER_EPOCH = 200   # 每一輪更新時, 要訓練幾步\n",
    "    9\n",
    "   10     BATCH_SIZE = 64\n",
    "   11     SOFT_UPDATE_RATE = 0.3\n",
    "   12\n",
    "   13     # --- 初始化 ---\n",
    "   14     replay_buffer = ReplayBuffer(capacity=REPLAY_BUFFER_CAPACITY)\n",
    "   15\n",
    "   16     # --- 階段 1: 預填充數據池 ---\n",
    "   17     print(\"--- 正在預填充數據池... ---\")\n",
    "   18     for _ in range(PREFILL_EPISODES):\n",
    "   19         episode_data = generate_episode_data()\n",
    "   20         replay_buffer.buffer.extend(episode_data)\n",
    "   21     print(f\"--- 預填充完成，數據池大小: {len(replay_buffer)} ---\")\n",
    "   22\n",
    "   23     # --- 階段 2: 主訓練迴圈 ---\n",
    "   24     for epoch in range(EPOCHS):\n",
    "   25\n",
    "   26         # 2a. 玩新的遊戲，並將數據加入數據池\n",
    "   27         for _ in range(EPISODES_PER_EPOCH):\n",
    "   28             episode_data = generate_episode_data()\n",
    "   29             replay_buffer.buffer.extend(episode_data)\n",
    "   30\n",
    "   31         # 2b. 從數據池中抽樣，進行訓練\n",
    "   32         for _ in range(TRAIN_STEPS_PER_EPOCH):\n",
    "   33             # 從 buffer 中抽樣一個 batch\n",
    "   34             b_state, b_action, b_reward, b_next_state, b_done = replay_buffer.sample(BATCH_SIZE)\n",
    "   35\n",
    "   36             # 將 numpy array 轉換為 tensor\n",
    "   37             b_state = torch.FloatTensor(b_state).reshape(-1, 4)\n",
    "   38             b_action = torch.LongTensor(b_action).reshape(-1, 1)\n",
    "   39             b_reward = torch.FloatTensor(b_reward).reshape(-1, 1)\n",
    "   40             b_next_state = torch.FloatTensor(b_next_state).reshape(-1, 4)\n",
    "   41             b_done = torch.LongTensor(b_done).reshape(-1, 1)\n",
    "   42\n",
    "   43             # 訓練兩個模型 (使用抽樣的 batch)\n",
    "   44             advantage = train_critic(b_state, b_reward, b_next_state, b_done)\n",
    "   45             loss = train_actor(b_state, b_action, advantage)\n",
    "   46\n",
    "   47             # 軟更新延遲網路\n",
    "   48             for param, param_delay in zip(model_critic.parameters(), model_critic_delay.parameters()):\n",
    "   49                 value = param_delay.data * (1.0 - SOFT_UPDATE_RATE) + param.data * SOFT_UPDATE_RATE\n",
    "   50                 param_delay.data.copy_(value)\n",
    "   51\n",
    "   52         # 定期評估並印出結果\n",
    "   53         if epoch % 100 == 0:\n",
    "   54             test_reward = test_agent_and_show() # 假設 test_agent_and_show() 函式已定義\n",
    "   55             print(f\"Epoch {epoch}, Buffer: {len(replay_buffer)}, Test Reward: {test_reward}\")\n",
    "   56\n",
    "   57 # --- 最終執行 ---\n",
    "   58 # train_main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
