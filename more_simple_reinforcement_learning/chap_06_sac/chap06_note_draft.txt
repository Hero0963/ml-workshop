SAC 算法

01. AC 算法的改進 S means soft

02. goal 本來是 max Q(s, a) 
改為 max Q(s, *) P(* | s) + \alpha H (Q(s, *))
引入 entropy + 改成考慮所有 action 並加權求和

03. 引入 entropy 的理由：
避免學習太劇烈(採樣機率極端化，某些動作的機率一下子降到 0)

----

看代碼

04. SAC 依然是同策略，但使用了 Pool (數據池)
因為已經是相當程度的解耦
(check code implement)

05. \alpha 如何自動調節
check this
target_entropy = log(action_dim) -> 上界
為經驗值法則

06. E = 1/2 log (2 * pi * e \sigma ^2) 
check 連續動作的 entropy 怎麼定義的

07. action = dist.rsample()
用法，訓練時怎麼做 bp


----

'''

重參數化技巧（Reparameterization Trick）     解決的問題： 在許多生成模型（如VAE）中，模型通常需要從某個分布（例如均值為 \(\mu \)、方差為 \(\sigma ^{2}\) 的正態分布）中採樣潛在變量 \(z\) 來生成數據。 然而，採樣操作本身是不可微的，這使得我們無法直接通過梯度下降來優化模型的參數。  實現方法： 重參數化技巧將採樣過程分解為兩步：  從一個固定的、獨立於模型參數的分布（通常是標準正態分布，即均值為0，方差為1的分布）中採樣一個隨機噪声 \(\epsilon \)。  使用模型學到的參數（如均值 \(\mu \) 和標準差 \(\sigma \)）對這個標準化的噪声 \(\epsilon \) 進行確定性的變換，將其映射到目標分布中的樣本 \(z\)。 例如，\(z=\mu +\sigma \cdot \epsilon \)。 

'''

 總結來說，唯一的區別就在於「是否需要為了訓練而回傳梯度」。如果需要，就用 `rsample()`；如果不需要，用 `sample()` 即可。

╭─────────────────────────╮
│  > 這邊的 r 是什麼意思  │
╰─────────────────────────╯

✦ 這個問題問得非常好！rsample 中的 `r` 就是 Reparameterization (重參數化) 的縮寫。

  這其實是 PyTorch 開發者在設計函式庫時，採用的一種非常簡潔且直觀的命名慣例。


  讓我們來看看他們當時的思考過程：

  1.  他們需要一個標準的、符合統計學定義的採樣方法，於是他們創造了 .sample()。這個方法就是單純地從機率分佈中回傳一個隨機值。


   2. 但是，在深度學習中，尤其是在訓練生成模型（如 VAE）或隨機策略（如
      SAC）時，他們迫切需要一種可微分的採樣方法，以便梯度能夠回傳。這個問題的解決方案，就是我們剛剛深入探討的「重參數化技巧 (Reparameterization
      Trick)」。
   3. 現在，他們需要為這個使用了「重參數化技巧」的新採樣方法取一個名字。他們有幾個選擇：
   4. 他們最終選擇了後者。在標準的 sample 前面，加上了其核心技術 Reparameterization 的首字母 `r`，於是就誕生了 rsample()。
   * 看到 .sample()，你就知道這是一個標準的、不可微分的採樣。
  這是一個將函式命名與其背後關鍵數學技巧直接綁定的絕佳範例。

╭──────────────────────────────────────────╮
│  > \mu   \sigma \epsilon 總共有三個參數  │
│    其中 \mu   \sigma 待定是嗎            │
│    \epsilon 是做什麼用的                 │
╰──────────────────────────────────────────╯

✦ 您問的問題完全切中了要害，也代表您對這個機制的理解已經非常深入了！

  > μ、σ、ε 總共有三個參數，其中 μ、σ 待定是嗎？ε 是做什麼用的？


  您的理解完全正確！我們來逐一拆解這三個參數的角色：

  ---

  ### `μ` (mu) 和 `σ` (sigma)：待定的「策略參數」


  您說的「待定」非常精準。μ 和 σ 正是我們的演員 (Actor) 神經網路需要透過訓練來學習和決定的。


   * `μ` (mu - 均值)：代表了演員在看到當前局面 (state) 後，認為最理想、最應該執行的動作。這是它的「最佳猜測」。
  訓練的目標：就是調整演員神經網路的內部參數，讓它在任何情況下，都能給出最優的 μ 和 σ 組合，以獲得最高的長期回報。

  ---

  ### `ε` (epsilon)：獨立的「隨機來源」

  ε 的角色非常單純且關鍵，它就是整個隨機性的唯一來源。


   * 它是什麼：ε 是一個從固定的、標準的高斯分佈 $\mathcal{N}(0, 1)$ 中隨機抽取的數字。這個分佈的中心永遠是 0，標準差永遠是
     1，它本身不會被訓練，也不會被改變。
   * 它做什麼用：它就像一個「隨機的推力」或「不可控的擾動」。我們來看這個公式：
  這樣做就把「策略（由 `μ` 和 `σ` 決定）」和「隨機性（由 `ε` 提供）」這兩件事情完美地解耦（分開）了。

  *   神經網路專心負責學習 μ 和 σ。
  *   而隨機抽樣的任務，則完全交給了獨立的、與網路無關的 ε。

  正是因為這種解耦，梯度才能順利地回傳給 μ 和 sigma，而不會被隨機抽樣過程打斷。

  ### 一個生動的比喻：射箭


   * 演員 (Actor)：一位正在學習射箭的弓箭手。
  弓箭手能訓練的，是他的「瞄準 (μ)」和「穩定性 (σ)」，但他無法訓練「風 (ε)」。這就是重參數化技巧的直觀體現。

  ----


  這個自動調節機制到底是如何運作的。


  我們之前的解釋是：「房間太熱（探索不足），溫控器（alpha）就調高」。這是一個高度簡化的比喻。真實的運作方式更像一個精密的動態平衡系統，我們
  必須同時觀察三個「玩家」的行為：演員 (Actor)、評論家 (Critic) 和 `alpha`。

  ---


  第一步：理解三個玩家各自的「目標」
   1. 演員 (Actor)：它的目標是最大化一個「綜合分數」，這個分數是「評論家給的動作分 `Q(s,a)`」和「探索獎勵 `alpha * H`」的總和。
   2. 評論家 (Critic)：它的目標是準確地預測出「綜合分數」。所以它學習的「標準答案 y」也必須包含探索獎勵。
   3. Alpha (`α`)：它的目標最單純，就是讓演員的平均「隨機性 `H`」盡量等於我們設定的「目標隨機性
      `H_target`」。它有自己的損失函數和優化器，專門負責這件事。
  第二步：當「探索不足」時，系統到底發生了什麼？
  場景：演員變得太「固執」，隨機性 (Entropy) 下降，低於了我們的目標。
   * train_alpha 函數被調用。
  這就是矛盾點：我們希望增加探索，但演算法反而降低了 alpha，這意味著「探索獎勵」的權重變小了。這看起來完全是反的！但請繼續看下一步。

  2. 評論家 (Critic) 的反應 (解開謎題的關鍵)


   * train_value 函數被調用。
  3. 演員 (Actor) 的反應 (最終的平衡)


   * train_action 函數被調用。
  結論：一個精妙的間接控制
   1. `alpha` 是市場利率：當系統發現「創新不足（熵低）」時，它不會直接給創新部門發獎金。
  這就是 SAC 自動調節 alpha 的精髓：它不直接控制策略的熵，而是通過調節 alpha
  來改變價值函數的形態，從而間接地引導策略朝著滿足熵約束的方向進行探索和優化，最終達到一個精妙的動態平衡。

╭────────────────────────╮
│  > alpha 怎自動麼調節  │
╰────────────────────────╯

✦ 好的，我換一種更直接、更聚焦於「程式碼如何實現自動調節」的角度來解釋。

  自動調節 alpha 的過程，可以分解為三個關鍵步驟，這在 03 和 04 兩個 Notebook 中是完全一樣的：

  ---


  第一步：將 `alpha` 提升為「可訓練參數」
   * 程式碼實現：
  `python
      # From Cell 10 in 03_discrete_auto_alpha.ipynb

      # 1. 將 alpha_log 定義為一個可訓練的參數
      alpha_log = torch.nn.Parameter(torch.FloatTensor([-1.0]))

      # 2. 為 alpha_log 建立一個專屬的優化器 (教練)
      optimizer_alpha = torch.optim.Adam([alpha_log], lr=1e-4)
      `

  ### 第二步：設定一個「目標熵」


  我們需要給演算法一個目標，告訴它我們期望的策略「隨機性」應該維持在什麼水平。這個目標就是 target_entropy。


   * 如何設定：
  `python
      # From Cell 13 in 03_discrete_auto_alpha.ipynb
      import math

      # 設定目標熵
      target_entropy = math.log(2) # 假設是2個動作的離散情況
      `

  ### 第三步：為 `alpha` 設計損失函數並更新


  這是整個自動調節機制的核心。我們為 alpha 設計了一個專屬的損失函數，然後用它的專屬優化器 optimizer_alpha 來最小化這個損失。


   * `alpha` 的損失函數：
   * 訓練流程：
   * 更新效果：
  自動調節 alpha 的本質，就是將 `alpha` 視為一個獨立的可訓練模型。它有自己的參數 (alpha_log)、自己的目標 (target_entropy)、自己的損失函數
  (loss_alpha) 和自己的優化器 (optimizer_alpha)。


  在主訓練循環中，我們不僅訓練演員和評論家，還同時訓練
  `alpha`，讓它根據策略的實際熵和我們的目標熵之間的差距，來動態地、自動地調整自己，從而實現了整個系統的自適應平衡。

╭─────────────────────────────────────╮
│  > target_entropy 是一個經驗法則嗎  │
╰─────────────────────────────────────╯

✦ 是的，您說得非常對！

  `target_entropy` (目標熵) 在很大程度上是一個基於「經驗法則 (Rule of Thumb)」或「啟發式 (Heuristic)」設定的超參數。


  目前並沒有一個通用的、理論上完美的公式可以計算出適用於所有問題的最佳
  target_entropy。它的設定反映了我們希望演算法達到的「探索」與「利用」的平衡點。

  不過，社群和原始論文的作者已經總結出了一些非常好用且廣被採納的經驗法則，作為設定初始值的絕佳起點。

  ---


  1. 離散動作空間 (Discrete Action Spaces)
   * 經驗法則：將目標熵設定為最大熵的一個固定比例，通常是一個較高的比例，例如 98%。
  `python
      import math
      # CartPole 有 2 個動作 (向左, 向右)
      action_dim = 2
      target_entropy = 0.98 * math.log(action_dim)
      `


  2. 連續動作空間 (Continuous Action Spaces)
   * 經驗法則：直接將目標熵設定為負的動作空間維度。
   1. 更直觀：設定一個「隨機性目標」比猜測一個神秘的「溫度係數」要容易理解得多。
  因此，自動調節 alpha 的機制，本質上是將一個難以調整的超參數 (`alpha`)，轉化為了一個相對容易設定的超參數
  (`target_entropy`)，這大大簡化了演算法的應用和調參過程。

  ----

