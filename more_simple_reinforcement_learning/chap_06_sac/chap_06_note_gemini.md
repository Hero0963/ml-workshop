# 強化學習講義 - Chap06：柔性致策評論家 (Soft Actor-Critic, SAC) (Gemini 整理版)

本章節將深入探討當前最先進、最受歡迎的強化學習演算法之一：**柔性致策評論家 (Soft Actor-Critic, SAC)**。SAC 是一個 Off-Policy 的 Actor-Critic 演算法，它以其出色的**樣本效率 (Sample Efficiency)**、**穩定性 (Stability)** 和在複雜連續控制任務上的卓越表現而聞名。

其最核心的思想，是在標準的強化學習目標（最大化累積回報）的基礎上，額外引入了**最大化策略熵 (Entropy Maximization)** 的目標。

---

## 1. 核心思想：最大化熵框架

傳統的強化學習演算法，其目標是學習一個能最大化期望回報的策略。然而，這樣的策略很容易過早地收斂到一個次優的、確定性的行為模式上，從而停止探索。

SAC 則對這個目標進行了修正，它旨在最大化一個包含了熵正則項的「軟」目標函數：

$$ 
J(\pi) = \sum_{t=0}^{T} \mathbb{E}_{(s_t, a_t) \sim \rho_\pi} \left[ r(s_t, a_t) + \alpha H(\pi(\cdot|s_t)) \right] 
$$ 

*   $r(s_t, a_t)$ 是我們熟悉的回報 (Reward)。
*   $H(\pi(\cdot|s_t))$ 是策略 $\pi$ 在狀態 $s_t$ 時的**熵 (Entropy)**，它衡量了策略的隨機性。熵越高，代表策略越傾向於探索。
*   $\alpha$ 是**溫度係數**，用於權衡回報和熵這兩個目標的重要性。

**引入熵的好處：**

1.  **增強探索**：直接獎勵策略的隨機性，能有效防止策略過早收斂，使其能探索更廣闊的狀態-動作空間。
2.  **提高穩定性**：熵項使得損失函數的地形更平滑，有助於穩定訓練過程，並降低對超參數設定的敏感度。

---

## 2. SAC 的關鍵架構

SAC 的實現包含三個關鍵的網路組件：

### A. 策略網路 (Actor)

負責根據當前狀態，輸出一個**隨機性策略**。

*   **離散動作空間 (如 CartPole)**：Actor 網路的輸出層使用 `Softmax`，直接輸出一個代表各動作選擇機率的**分類分佈 (Categorical Distribution)**。
*   **連續動作空間 (如 Pendulum)**：Actor 網路通常有兩個輸出頭，分別輸出一個**高斯分佈 (Gaussian Distribution)** 的**均值 $\mu$** 和**標準差 $\sigma$**。

### B. 價值網路 (Critic)

負責評估狀態-動作的價值。SAC 採用了與 TD3 類似的 **Clipped Double Q-Learning** 技巧。

*   **雙 Q 網路**：同時訓練兩個結構相同但獨立的 Q 網路 ($Q_{\phi_1}, Q_{\phi_2}$)。
*   **目標**：在計算目標 Q 值時，總是取兩個網路中較小的那個估計值，以有效緩解 Q 值的過高估計問題。

### C. 目標價值網路 (Target Value Networks)

*   為了穩定訓練，兩個 Q 網路都有其對應的**目標網路** ($Q_{\phi_1^-}, Q_{\phi_2^-}$)。
*   目標網路的參數**不透過梯度下降更新**，而是通過**軟更新 (Polyak Averaging)** 來緩慢地追蹤主網路的參數：
    $$ 
    \theta_{\text{target}} \leftarrow \tau \theta_{\text{online}} + (1 - \tau) \theta_{\text{target}} 
    $$ 
    其中 $\tau$ 是一個很小的常數（例如 0.005）。

---

## 3. 核心問題釐清

### A. On-Policy vs. Off-Policy

*   **嚴格定義**：SAC 是一個標準的 **Off-Policy (異策略)** 演算法。
*   **判斷依據**：判斷的核心在於**更新模型時所用的數據來源**。SAC 從一個巨大的**經驗回放池 (Replay Buffer)** 中隨機抽樣數據來進行訓練，而這個池中的數據可能來自各種新舊策略。這種能夠利用「非當前策略」所產生數據的特性，正是 Off-Policy 的定義。
*   **為何能用數據池**：SAC 的貝爾曼方程更新不依賴於產生數據的原始策略，因此可以高效地重複利用過去的經驗，這也是它樣本效率高的主要原因。

### B. `rsample()` vs `sample()`：可微分的秘密

*   **挑戰**：在連續動作空間中，從一個機率分佈中「採樣」這個動作本身是隨機的，梯度無法通過。
*   **解決方案：重參數化技巧 (Reparameterization Trick)**，這正是 `rsample()` 的作用。
*   **機制**：它將採樣過程 $a \sim \mathcal{N}(\mu, \sigma)$ 變換為一個確定性函數 $a = \mu + \sigma \times \epsilon$，其中 $\epsilon \sim \mathcal{N}(0, 1)$ 是一個固定的外部噪聲源。
*   **結果**：`action` `a` 因此與網路的輸出 `μ` 和 `σ` 建立了一條**可微分的連接**，使得梯度可以在反向傳播時順利通過，從而能夠訓練輸出分佈參數的策略網路。

---

## 4. `alpha` 的自動調節機制

手動設定並衰減 `alpha` 是一個棘手的超參數問題。SAC 的一個重大貢獻是提出了一種自動調節 `alpha` 的機制。

*   **核心思想**：將 `alpha` 從超參數變為一個可訓練的參數，並為它設定一個優化目標：**讓策略的平均熵盡可能接近一個預設的目標熵 `target_entropy`**。
*   **實現方式**：
    1.  定義一個可訓練的參數 `log_alpha`，並為其創建專屬的優化器。
    2.  設定一個目標熵 `target_entropy`。這是一個基於**經驗法則**設定的超參數，但比設定 `alpha` 本身更直觀。
        *   **離散空間**：通常設為最大熵的某個比例，如 $0.98 \times \log(\text{action\_dim})$。
        *   **連續空間**：通常設為 $-\text{action\_dim}$。
    3.  為 `alpha` 定義損失函數：
        $$ 
        L(\alpha) = -\mathbb{E}_{s_t \sim \mathcal{D}} [\alpha_{\log} (H(\pi_t(\cdot|s_t)) - \mathcal{H}_{\text{target}})] 
        $$ 
*   **動態平衡**：在訓練中，如果實際熵低於目標熵，該損失函數會驅使 `alpha` 的值發生變化，進而改變 Critic 的目標 Q 值和 Actor 的更新目標，間接地引導 Actor 增加策略的隨機性，反之亦然，最終使系統達到一個動態的平衡。
