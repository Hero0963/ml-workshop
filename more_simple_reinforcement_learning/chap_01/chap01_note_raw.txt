ref_link: https://www.bilibili.com/video/BV1ZwZtYtE4R

強化學習是一種機器學習方法，目標是讓智能體（Agent）在環境（Environment）中透過試誤（Trial and Error）方式學習最佳策略（Policy）。
智能體根據當前狀態（State）選擇一個動作（Action），然後從環境中獲得回饋（Reward）與新的狀態。藉由最大化累積報酬（Cumulative Reward），
智能體逐步學習出最佳的決策行為。

強化學習方法只要求結果，中間過程讓 agent 自行規劃

求解過程與遊戲環境複雜度無關


冰湖 4 x 4 種狀態
反饋表、狀態表、策略表
Q 表 所有狀態下所有動作的分數(指導 Agent 在每一個狀態下應該做什麼動作)

流程: start a new game -> state, cur_pos, action, feedback, arrive_pos, game_end? 
calculate total feedback -> update Q table

Q 表適合狀態空間小的問題（像 4x4 冰湖）。
狀態太多時（例如玩 Atari 遊戲），會用 Q Network（DQN）取代表格，改用神經網路逼近 Q 值。


# 强化学习从原理到实践 第2章 Q函数和时序差分
to_deconfuse: Q table update process 
update Q table 的方式: 玩完一局後，有拿到禮物(or 反饋總和/ 總分 比以往好)。，就把當時狀態選擇的操作全部加分！
加多少分? => 自己設定

SARSA（State-Action-Reward-State-Action）


時序差分算法 -> 走一段路就進行估計
不一定要走完全程才能優化Q表


# 須注意是異策略（off-policy） or 同策略（on-policy）(只能使用自己產生的數據)算法
# off-policy(觀戰) - QLearning、DQN、DDPG、TD3、SAC 
# on-policy(自己復盤) - Sarsa、PPO 


# update QTable by TD
Q(s_t, a_t) = E[R_t + gamma * R_{t + 1} + gamma^2 * R_{t + 2} + .... + gamma^{n-t} * R_n]
Q(s_t, a_t) = R_t + E[gamma * R_{t + 1} + gamma^2 * R_{t + 2} + .... + gamma^{n-t} * R_n]
Q(s_t, a_t) = R_t + gamma * Q(s_{t+1}, a_{t + 1})

value = Q(s_t, a_t)
target = R_t + gamma * Q(s_{t+1}, a_{t + 1})
td = target - value
Q(s_t, a_t) <- Q(s_t, a_t) + alpha * td




QLearning:
Q(s_t, a_t) = R_t + gamma * max_v -> Q(s_{t+1}, *)


'''
Sarsa：你下次也是用你「實際」選的方向來更新 Q 值。

Q-Learning：你假設下一步會選分數最高的方向來更新 Q 值，不管你實際上選的是不是那個方向。
'''


value 和target 应该會相等

target = reward + Q[next_state].max() * 0.9
這個 * 0.9 衰減是自己定義的(discount factor, γ, 0 < γ < 1, usually pick  γ = 0.9)

這種設置下，Q-Learning 的 Bellman 方程一定有唯一解，Q 值會收斂到一個固定點（即最終值）。
貝爾曼方程就是：「現在的價值 = 現在的分數 + 未來所有最佳選擇帶來的期望分數（有折扣）

target 是「根據新觀察到的結果或合理預測算出來的最新正確答案」
value（Q 表）只是「你之前的猜測」
TD (Temporal Difference) 學習核心就在於用 target 來糾正 value。

---------------------------------


