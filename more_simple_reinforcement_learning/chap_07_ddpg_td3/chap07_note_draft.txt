DDPG, TD3

01. 通常用在連續動作空間(比較好更新)
explain why

02. 以前的做法 Q 值容易高估
why 高估而不是低估
因為更新方向是 pick max (Q value ?)
當 model_value 打分錯誤時，model_action 會去迎合

_釐清 Q, value, target, critic

03. 改善方式：引入 model_delay

04. DDPG 思想，輸出要穩定
並非靠取 max
而是直接輸出一個動作(以前是一組概率分布)
_check code implement


  🧑‍🏫 高中生版本解說

  想像一下，演員 (Actor) 的學習方式是靠評論家 (Critic) 給他「指引方向」。

   * 在連續動作空間（DDPG 的主場）：
       * 演員的動作就像一個可以平滑轉動的旋鈕（比如音量旋鈕）。
       * 評論家看完演員的動作後，可以給出一個非常精確的指令：「你把旋鈕順時針再轉一點點，分數會更高。」
       * 這個「再轉一點點」的指令是連續的、平滑的，演員可以很直接地根據這個指令微調自己的動作。

   * 在離散動作空間（DDPG 的客場）：
       * 演員的動作就像一個電燈開關，只有「開」和「關」兩個選項。
       * 現在，評論家沒法給出「再開一點點」這種指令了，因為不存在「半開」的狀態。
       * DDPG 的那套「給你一個平滑的調整方向」的核心學習機制，在這裡完全失效了。你無法對一個「開/關」的選擇直接進行「微調」。

  結論就是：DDPG 的學習方法是為了「微調旋鈕」而設計的，你硬要讓它去學怎麼「按開關」，它就會水土不服。與其這麼麻煩，我們有更好的工具專門用來
  學「按開關」，那就是 DQN。DQN 直接計算「開」和「關」兩個選項的分數，然後選分數高的那個，簡單又高效。

  ---

  👨‍💻 專業術語解說

  DDPG 不適用於離散動作空間的根本原因在於其梯度計算的路徑是不可微的 (non-differentiable)。

   1. DDPG 的梯度路徑:
      DDPG 的策略梯度更新依賴於鏈式法則，梯度從 Critic 的輸出 $Q_\phi(s,a)$，通過動作 $a$，回傳到 Actor 的參數 $\theta$。其梯度公式為：
      $$
      \nabla_\theta J \approx \mathbb{E}_s \left[ \nabla_a Q_\phi(s,a)|_{a=\mu_\theta(s)} \cdot \nabla_\theta \mu_\theta(s) \right]
      $$
      這個公式成立的前提是：動作 $a = \mu_\theta(s)$ 必須是 Actor 網路參數 $\theta$ 的一個可微函數。在連續空間中，Actor
  輸出一個浮點數，這個條件是滿足的。

   2. 離散空間中的問題:
      在離散動作空間中，Actor 需要從一組有限的動作（例如 {0, 1, 2}）中選擇一個。最直觀的做法是讓網路輸出每個動作的 Q 值或
  logits，然後通過一個 argmax 操作來選出動作。
      $$
      a = \text{argmax}(\text{Actor_output})
      $$
      然而，`argmax` 函數是不可微分的。它的梯度在幾乎所有地方都是 0，在決策邊界上則沒有定義。這意味著，來自 Critic 的梯度 $\nabla_a Q$
  在這裡完全無法回傳給 Actor 的參數 $\theta$，梯度路徑被切斷了，Actor 無法學習。

   3. 可能的「魔改」與其代價:
       * Gumbel-Softmax 技巧: 為了讓離散選擇變得可微，研究者們發明了 Gumbel-Softmax 這種技巧。它可以產生一個可微分的、接近 one-hot
         編碼的向量來代表離散選擇。你可以用它來改造 DDPG，但這樣做會讓演算法變得非常複雜，並且本質上是把確定性策略又變回了某種形式的隨機策略。
       * 效率問題: 即使魔改成功，其樣本效率和穩定性通常也遠不如專為離散空間設計的演算法。

   4. 更優越的替代方案:
       * 價值學習方法 (Value-Based): DQN 及其變體 (Double DQN, Dueling DQN, Rainbow)
         是處理離散動作空間的王者。它們直接為每一個離散動作估計一個 Q 值，然後通過 argmax 選擇最優動作。這個 argmax
         操作只在執行策略時使用，而不在梯度計算路徑上，完美地避開了不可微的問題。
       * 隨機策略梯度 (Stochastic Policy Gradient): A2C/PPO 也可以處理離散動作空間。它們讓 Actor 輸出一個機率分佈 (e.g., via
         Softmax)，然後使用 log-derivative trick ($\nabla_\theta \log \pi_\theta(a|s)$) 來計算梯度，這條梯度路徑也是完全可微的。



 條目 4: _釐清 Q, value, target, critic

  🧑‍🏫 高中生版本解說
  這是在提醒自己搞清楚幾個關鍵名詞，我們把它們對應到程式碼和角色上：
   * `Critic`: 指的是「評論家」這個角色，在程式碼中就是 model_value 這個神經網路。
   * `Q` / `Q-value`: 這是「Q值」，是 Critic 網路對某個「狀態-動作」組合給出的具體分數。
   * `value`: 在程式碼的 train_value 函式中，value 這個變數通常用來儲存 Critic 對當前數據批次做出的預測分數。
   * `target`: 指的是我們用來訓練 Critic 的「標準答案」。它的計算方法是「這一步的真實獎勵 + 打了折扣的、由替身評論家預測的下一步分數」。

  👨‍💻 專業術語解說
  這是一次術語對應的梳理：
   * `Critic`: 指的是價值函數近似器，即神經網路 $Q_\phi(s,a)$。在程式碼中對應 `model_value`。
   * `Q` / `Q-value`: 指的是動作-價值函數 (Action-Value Function) 的輸出，是對在狀態 $s$ 採取動作 $a$ 後的期望回報的估計。
   * `value`: 在程式碼上下文中，這通常是指由主網路 (Online Network) $Q_\phi$ 計算出的當前 Q 值估計，即 $Q_\phi(s_t, a_t)$。
   * `target`: 指的是用於監督 Critic 學習的 TD-Target。它由目標網路 (Target Networks) 計算得出，公式為 $y_t = r_t + \gamma Q'{\phi}(s{t+1},
     \mu'{\theta}(s{t+1}))$。