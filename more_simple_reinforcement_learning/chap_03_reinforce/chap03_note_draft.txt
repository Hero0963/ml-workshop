ref = https://www.bilibili.com/video/BV1FZZ4YwE84

# 策略梯度
回顧：ＱLearning, SARSA, DQN 基於價值
從不好的狀態移動到好的狀態，並採取好的動作
核心 = 優化 Q Table

now: 基於策略做決策
state -> model -> p(action) 
output= 採取某個動作的機率

# 策略迭代 
goal: 找出最大收益的策略
修改做決策的概率


t 時刻: S_t, A_t, R_t 
t + 1 時刻: S_{t + 1}, A_{t + 1}, R_{t + 1} 
t + 2 時刻: S_{t + 2}, A_{t + 2}, R_{t + 2} 

U_t = R_t + gma R_{t + 1} + gma^2 R_{t+2} + ... + gma^{n - t} R_n
Q(s_t, a_t) = E[U_t | S_t=s_t, A_t=a_t]



----


RL note
01. 基於策略做決策 輸出 -> 採取某個動作的機率

02. 基於價值做決策 -> 選最高分的動作

03. 以冰湖遊戲為例，探索  get a path  (strategy) ex: 右右右下下右

04. goal: 處於某一狀態時，輸出採取的策略使其最大化後續的折扣回報和
實務計算: 蒙地卡羅採樣

05. Reinforce: 最基本的基於策略的算法

06. 去基線版本 (U - U\bar) * lnP(action) 更穩定

07. 實作:
model_action: 計算每個動作的概率
model_baseline: 評估 state 的價值

08. 熵正則 H(x) = -[p(x)ln(p(x)) + q(x)ln(q(x))], where q(x) = 1 - p(x)
衡量動作選擇的不確定性

09. 引入 entropy in Loss function
讓模型收斂變慢，原本的反饋會使模型很快就不選擇該動作
讓訓練的效率降低，但穩定性增加

----

 好的，我們來逐條深入解析這幾點，這幾點是從 REINFORCE 演算法演進到更現代的 Actor-Critic 方法的核心。

  ---

  06. 去基線版本 (U - U_bar) * lnP(action) 更穩定

   * 逐字解說:
       * lnP(action): 這是策略梯度的核心，代表我們對「實際採取的那個動作」的置信度取對數。我們的目標是調整模型參數，來改變這個值。
       * U: 代表 Utility 或 Return，也就是我們在 01_reinforce.ipynb 裡看到的
         value。它是通過蒙地卡羅方法計算出的，從當前這一步開始，直到遊戲結束的「實際未來總獎勵」。
       * U_bar: 這個 "bar" 代表平均值 (average)。它就是由 model_baseline 預測出的，在當前狀態下「預期或平均的未來總獎勵」。
       * (U - U_bar): 這就是優勢 (Advantage)。它回答的問題不是「我這次得分高嗎？」，而是「我這次的得分比我預期的要高多少？」。
   * 為什麼「更穩定」?
      這解決了 REINFORCE 最大的痛點：高變異數 (high variance)。
       * 沒有 Baseline 的情況: 假設在一局很棒的遊戲中，總分是 200。那麼幾乎每一步的 U (未來總獎勵) 都是一個很大的正數。這會導致模型認為這局裡
         的所有動作都是好的，並全部給予獎勵。但實際上，其中可能有一兩步是臭棋，只是運氣好沒造成災難。

       * 有 Baseline 的情況: 假設在某一步，model_baseline 預測平均能得 50 分 (U_bar = 50)。但因為後續的幾步走得特別好，實際的未來總獎勵是 80
         分 (U = 80)。那麼優勢就是 80 - 50 = +30，模型會得到一個清晰的「比預期好」的正向訊號。反之，如果後續走得不好，實際只得了 10 分 (U =
         10)，那麼優勢就是 10 - 50 = -40，模型會得到一個強烈的「遠遜於預期」的負向訊號，即使 10 分本身是正數。
       * 結論: Baseline
         將一個絕對的、充滿噪聲的獎勵訊號，轉換成了一個相對的、更乾淨的「優勢」訊號，讓每一次的更新都更有意義，從而使訓練過程極大地穩定下來。

  ---

  07. 實作: model_action 和 model_baseline

   * 解說:
      這條筆記點出了實現「去基線版本」的架構。我們需要兩個獨立的模型，這其實就是 Actor-Critic (演員-評論家) 架構的雛形。
       * model_action (策略網路) 扮演 演員 (Actor) 的角色。它的職責是觀察環境 (state)，然後做出決策 (輸出動作機率 P(action))。它負責表演。
       * model_baseline (價值網路) 扮演 評論家 (Critic) 的角色。它的職責也是觀察環境 (state)，但它不做決策，而是給出一個評價 (預測狀態的價值
         U_bar)。它負責打分。
   * 兩者如何協作:
       1. 演員 (`model_action`) 上場表演 (玩一局遊戲)。
       2. 表演結束後，我們計算出演員的實際得分 (`U`)。
       3. 評論家 (`model_baseline`) 發表意見，給出它對演員在各個狀態下的預期得分 (`U_bar`)。
       4. 我們用「實際得分」減去「預期得分」得到優勢 (`U - U_bar`)。
       5. 根據這個優勢，我們去指導演員 (`model_action`) 調整它的表演策略。
       6. 同時，我們也根據「實際得分」，去訓練評論家 (`model_baseline`)，讓它的打分越來越準。

  ---

  08. 熵正則 H(x) = -[p(x)ln(p(x)) + q(x)ln(q(x))] ...

   * 解說:
       * 熵 (Entropy) 在這裡是衡量模型決策的「不確定性」或「隨機性」的數學工具。
       * 一個極端的例子:
           * 低熵 (非常確定): 模型輸出 [向左: 99.9%, 向右: 0.1%]。模型非常固執，幾乎只會選擇向左。它的決策非常確定，所以熵值很低。
           * 高熵 (非常不確定): 模型輸出 [向左: 50%, 向右: 50%]。模型完全不知道哪個更好，處於最混亂、最不確定的狀態，所以熵值最高。
       * 在強化學習中，高熵等同於高探索性。一個高熵的策略會願意嘗試各種不同的動作。

  ---

  09. 引入 entropy in Loss function ...

   * 解說:
      這條筆記解釋了為什麼以及如何使用熵。
       * 為什麼引入？
         為了防止模型過早收斂。如果模型在訓練初期偶然發現一個還不錯的策略（比如一直向右就能活很久），它可能會迅速地把「向右」的機率調整到接近
         100%。這樣一來，它就再也不會去嘗試「向左」了，可能因此錯過了真正最優的、需要左右結合的策略。
       * 如何引入？ 我們在 loss 中加入熵的獎勵項 loss -= entropy * 5。
           * 優化器的目標是最小化 loss。
           * 我們希望最大化 entropy 來鼓勵探索。
           * 因此，當我們從 loss 中減去一個正的熵值時，就等於告訴優化器：「嘿，一個熵值更高的策略，它的 loss 會更低，是更好的！」
       * 效果 (效率 vs. 穩定性):
           * 讓模型收斂變慢，訓練效率降低: 這是因為模型現在有兩個目標：1. 提高分數。2.
             保持探索。它不能再像以前一樣一條路走到黑，需要花費更多的時間去「左顧右盼」，所以收斂到最終策略的速度變慢了。
           * 但穩定性增加: 這種「左顧右盼」的探索行為，大大降低了模型陷入局部最優解的風險。它看得更廣，更有可能找到全局最優解，因此訓練過程更
             穩定，最終得到的策略也更魯棒 (robust)。

╭──────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│  > "熵正則 H(x) = -[p(x)ln(p(x)) + q(x)ln(q(x))], where q(x) = 1 - p(x)" 為什麼這樣定義，請給我直觀解釋  │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────╯

✦ 好的，這個問題非常棒！我們不用深入到信息論的數學證明，而是用一個直觀的例子來理解為什麼這個公式能完美地衡量「不確定性」。

  核心思想：意外的程度

  熵的本質，可以理解為「平均意外程度」或者「平均驚訝程度」。

   * 一個非常確定的事件，發生了，你一點也不意外。
   * 一個非常不確定的事件，發生了，你會感到很意外。

  數學家們需要一個函數 I(p) 來量化「意外程度」，這個函數需要滿足幾個直觀的特性：

   1. 確定事件，意外為零: 如果一個事件發生的機率 p=1，那它發生時，意外程度應該是 I(1) = 0。
   2. 機率越小，越意外: 如果一個事件發生的機率 p 很小，那它發生時，意外程度 I(p) 應該很大。
   3. 獨立事件，意外相加: 如果兩個獨立事件同時發生（比如拋硬幣正面朝上 並且 骰子擲出6），總的意外程度應該是兩個事件意外程度的總和。

  神奇的是，log 函數（特別是 log(1/p) 或 -log(p)）完美地滿足了以上所有特性！

   * -log(1) = 0 (滿足特性1)
   * 如果 p 趨近於 0，-log(p) 趨近於無窮大 (滿足特性2)
   * -log(p1 * p2) = -log(p1) + -log(p2) (滿足特性3)

  所以，我們就用 -log(p) 來定義單個事件發生的「意外程度」。

  ---

  從「意外程度」到「熵」

  現在我們知道了單個事件的意外程度，熵 (Entropy) 就是所有可能事件的「平均意外程度」。

  「平均值」在數學上就是「期望值」，計算方法是：事件的價值 × 事件發生的機率，然後全部加起來。

  在這裡，「事件的價值」就是「意外程度」。

  讓我們以拋硬幣為例，p(x) 是正面朝上的機率，q(x) = 1 - p(x) 是反面朝上的機率。

   * 正面朝上這件事的「意外程度」是: -log(p(x))
   * 反面朝上這件事的「意外程度」是: -log(q(x))

  現在計算平均意外程度 (熵)：

  H(x) = (正面朝上的意外程度 × 正面朝上的機率) + (反面朝上的意外程度 × 反面朝上的機率)
  H(x) = (-log(p(x)) * p(x)) + (-log(q(x)) * q(x))
  H(x) = -[p(x)log(p(x)) + q(x)log(q(x))]

  這就是熵公式的由來！

  ---

  讓我們代入數字，看看直觀結果

   1. 極度確定的情況 (作弊的硬幣)
       * p(正面) = 0.99, q(反面) = 0.01
       * H = -[0.99 * log(0.99) + 0.01 * log(0.01)]
       * log(0.99) 約等於 0，所以第一項很小。
       * log(0.01) 雖然是個比較大的負數，但它只乘以了很小的 0.01。
       * 結果 H 非常接近 0。
       * 直觀解釋: 你幾乎總是能猜到結果是正面，所以平均下來，你一點也不意外。不確定性極低。

   2. 極度不確定的情況 (公平的硬幣)
       * p(正面) = 0.5, q(反面) = 0.5
       * H = -[0.5 * log(0.5) + 0.5 * log(0.5)] = -log(0.5)
       * 結果 H 約等於 0.693 (如果用自然對數 ln)。這是這種情況下能達到的最大值。
       * 直觀解釋: 你完全沒法猜結果，正面和反面的可能性一樣大。每次開獎你都懸著一顆心，平均下來，你的意外程度最高。不確定性極高。

  總結:

  這個公式 H(x) = -Σ p(x)log(p(x))
  之所以能衡量不確定性，是因為它精確地計算了所有可能結果的「加權平均意外程度」。當系統的結果越難預測，這個平均意外程度（熵）就越高。

╭─────────────────────────────────────────────────────╮
│  > Actor-Critic (演員-評論家) 架構 跟 GAN 的關係呢  │
╰─────────────────────────────────────────────────────╯

✦ 這是一個非常深刻的問題！您察覺到了這兩種架構在結構上的相似性，這正是許多研究者感到興奮的地方。它們的關係可以總結為：

  Actor-Critic 和 GAN 都是基於「兩個神經網路對抗性合作」思想的框架，但它們玩的「遊戲」目標和規則完全不同。

  讓我們用一個直觀的類比來拆解這一切。

  ---

  1. 核心的共同點：一個「兩位玩家的遊戲」

  想像一下，這兩種架構都是一個遊戲，裡面有兩位玩家（兩個神經網路），他們互相影響、共同進化。

   * 玩家A 的任務是產出某樣東西。
   * 玩家B 的任務是評價玩家A的產出。
   * 玩家A 根據玩家B的評價，來調整自己的產出策略，力求下次獲得更好的評價。
   * 玩家B 也會根據「真實的標準答案」，來提升自己評價的準確性。

  這個動態的訓練循環是兩者最核心的相似之處。但是，遊戲的內容和目標截然不同。

  ---

  2. Actor-Critic：演員與導演的「合作遊戲」

   * 遊戲目標: 合作拍出一部能獲得最高票房 (Reward) 的電影。
   * 玩家 (網路):
       * 演員 (Actor / `model_action`): 負責表演。看到劇本（狀態 s），決定要用哪種演技（動作 a）。
       * 導演 (Critic / `model_baseline`): 負責評價。他看著演員的表演（狀態 s），根據自己的經驗判斷這場戲大概能值多少分（價值 V(s)）。
   * 互動方式 (偏向合作):
       1. 演員演了一場戲，最終電影片段的實際效果 (`G_t`) 出來了。
       2. 導演給出他的預期分數 (`V(s)`)。
       3. 如果 實際效果 > 預期分數，導演會告訴演員：「演得好！比我想的還好！繼續保持這個感覺。」演員就會更傾向於這樣演。
       4. 如果 實際效果 < 預期分數，導演會說：「不行，這段戲沒達到應有的水準。」演員就會調整他的演法。
       5. 導演自己也會根據實際效果，反思自己的判斷，讓自己的評價能力越來越準。
   * 「真實標準」來源: 來自外部環境的回報 (Reward)，也就是最終的票房。

  ---

  3. GAN：偽畫家與鑑定師的「零和遊戲」

   * 遊戲目標: 偽畫家想畫出能騙過鑑定師的假畫；鑑定師想準確地鑑定出真假。
   * 玩家 (網路):
       * 偽畫家 (Generator): 負責創造。他憑空（或根據噪聲）畫出一幅看起來像真跡的假畫。
       * 鑑定師 (Discriminator): 負責鑑別。他接收一幅畫，判斷這幅畫是「真的」（來自真實數據集）還是「假的」（來自偽畫家）。
   * 互動方式 (純粹對抗):
       1. 偽畫家畫了一幅假畫，送給鑑定師。
       2. 鑑定師同時也看了一批真畫。
       3. 鑑定師給出判斷：「這是假的！」
       4. 偽畫家的目標: 調整自己的畫技，讓鑑定師下次判斷成「這是真的！」
       5. 鑑定師的目標: 調整自己的眼力，下次更準確地判斷出「這是假的！」
   * 「真實標準」來源: 來自一個固定的真實數據集（例如，幾萬張真實的貓咪照片）。

  ---

  總結對比


  ┌──────────┬─────────────────────────────────┬─────────────────────────────────────────────┐
  │ 特性     │ Actor-Critic (演員-評論家)      │ GAN (生成對抗網路)                          │
  ├──────────┼─────────────────────────────────┼─────────────────────────────────────────────┤
  │ 目標     │ 找到最優策略，最大化環境回報    │ 生成與真實數據無法區分的假數據              │
  │ 玩家     │ Actor (策略) & Critic (價值)    │ Generator (生成器) & Discriminator (判別器) │
  │ 互動性質 │ 合作指導 (Critic 指導 Actor)    │ 零和對抗 (Generator 欺騙 Discriminator)     │
  │ 反饋來源 │ 來自外部環境的動態獎勵 (Reward) │ 來自一個固定的真實數據集                    │
  │ 最終產物 │ 一個策略模型 (教你如何行動)     │ 一個生成模型 (教你如何創造)                 │
  └──────────┴─────────────────────────────────┴─────────────────────────────────────────────┘

  驚人的交叉點：GAIL

  更有趣的是，這兩種思想可以結合！最著名的例子就是 生成對抗模仿學習 (Generative Adversarial Imitation Learning, GAIL)。

   * 目標: 讓 AI 模仿專家的行為（比如模仿人類開車）。
   * 做法:
       * Actor 就像 Generator，它不生成圖片，而是生成「行為軌跡」（一連串的駕駛操作）。
       * Discriminator 就像 Critic，但它不評價分數，而是判斷這個「行為軌跡」是來自人類專家還是來自 Actor。
       * Actor 的「獎勵」不再是環境給的，而是它成功騙過 Discriminator 的程度！

  這完美地將 GAN 的對抗訓練思想，用來為強化學習的 Actor 提供學習訊號，解決了沒有明確獎勵函數的模仿學習問題。

╭────────────────────╮
│  > 哪個先提出來的  │
╰────────────────────╯

✦ Actor-Critic 架構的思想提出得更早。

  這是一個有趣的思想史演進過程：

   1. Actor-Critic 的早期思想 (1983年):
       * Actor-Critic 的核心概念——一個部分負責選擇動作（Actor），另一個部分負責評價這些動作（Critic）——最早可以追溯到 1983年 Andrew Barto,
         Richard Sutton 和 Charles Anderson 的論文《Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems》。
       * 這篇論文介紹了 AHC (Adaptive Heuristic Critic) 演算法，被廣泛認為是現代 Actor-Critic 方法的奠基之作。
       * 不過，早期的 Actor-Critic 方法受限於當時的計算能力和神經網路技術，並沒有立即產生巨大影響。直到深度學習時代，它才被重新發掘並與深度神
         經網路結合，形成了我們今天熟知的 A2C, A3C 等強大演算法。

   2. GAN 的提出 (2014年):
       * 生成對抗網路 (GAN) 的概念則是在 2014年 由 Ian Goodfellow 和他的同事們在一篇名為《Generative Adversarial Nets》的開創性論文中提出的。
       * GAN 的思想一經提出，就因其生成高質量圖像的驚人能力而在機器學習領域引起了轟動，並迅速成為深度學習中最熱門的研究方向之一。

  時間線總結:

   * 1980年代: Actor-Critic 的基本思想已經萌芽。
   * 1990年代 - 2000年代: 強化學習領域持續發展，但 Actor-Critic 仍相對小眾。
   * 2014年: GAN 橫空出世，引爆了生成模型領域。
   * 2015年後: 隨著深度學習的成熟，特別是 DeepMind 在 DQN 上的成功，研究者們重新將深度神經網路應用於 Actor-Critic 框架，誕生了 DDPG (2015),
     A3C (2016) 等著名演算法，Actor-Critic 迎來了復興和繁榮。

  所以，儘管我們現在經常將兩者相提並論，但從歷史上看，Actor-Critic 的思想比 GAN 早了整整三十年。GAN
  的成功，反過來也可能啟發了研究者們去重新審視和探索各種基於「兩個網路互動」的學習框架。